{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3a6942b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (0.11.7)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (2.0.44)\n",
      "Requirement already satisfied: psycopg2-binary in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (2.9.11)\n",
      "Requirement already satisfied: pandas in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: dotenv in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (0.9.9)\n",
      "Requirement already satisfied: requests in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from pdfplumber) (12.0.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (3.4.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (46.0.3)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from sqlalchemy) (3.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from sqlalchemy) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from pandas) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from dotenv) (1.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from requests) (2025.10.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\joão pedro\\desktop\\leitor-fopag\\.venv\\lib\\site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber sqlalchemy psycopg2-binary pandas dotenv requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e2ee287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexão com PostgreSQL estabelecida com sucesso no schema 'FOPAG'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Carrega as variáveis do .env\n",
    "load_dotenv()\n",
    "\n",
    "# --- Carregue os componentes individuais ---\n",
    "API_TOKEN = os.getenv('SOLIDES_API_TOKEN')\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASS = os.getenv('DB_PASS')\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_PORT = os.getenv('DB_PORT')\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "\n",
    "# (Opcional) Seu schema\n",
    "DB_SCHEMA = os.getenv('DB_SCHEMA') \n",
    "\n",
    "# --- Construa a DB_URL aqui no Python ---\n",
    "if not all([DB_USER, DB_PASS, DB_HOST, DB_PORT, DB_NAME]):\n",
    "    print(\"Erro: Faltando uma ou mais variáveis (DB_USER, DB_PASS, etc) no .env\")\n",
    "    exit()\n",
    "\n",
    "DB_URL = f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "# ------------------------------------------\n",
    "\n",
    "# Configurações globais da API\n",
    "# (A linha \"DB_URL = DB_URL\" era redundante e foi removida)\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Token token={API_TOKEN}\",\n",
    "    \"Accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Cria a \"engine\" de conexão com o banco\n",
    "try:\n",
    "    # Opção para definir o schema padrão da conexão\n",
    "    engine = create_engine(\n",
    "        DB_URL,\n",
    "        connect_args={'options': f'-csearch_path={DB_SCHEMA}'}\n",
    "    )\n",
    "    print(f\"Conexão com PostgreSQL estabelecida com sucesso no schema '{DB_SCHEMA}'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao conectar ao PostgreSQL: {e}\")\n",
    "    # Imprime a URL sem a senha para depuração\n",
    "    print(f\"String de conexão tentada: postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcc714a",
   "metadata": {},
   "source": [
    "# Fase 1: Pipelines das dimensões (Dados da API)\n",
    "\n",
    "## Passo 1.A: Pipelie da ```dim_departamentos```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d23993f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando ETL...\n",
      "Conexão estabelecida no schema 'FOPAG'.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pipeline_dim_calendario' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 418\u001b[39m\n\u001b[32m    416\u001b[39m \u001b[38;5;66;03m# --- EXECUÇÃO ---\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m     \u001b[43mpipeline_dim_calendario\u001b[49m() \u001b[38;5;66;03m# (Função mantida igual, omitida por brevidade, mas deve estar no código)\u001b[39;00m\n\u001b[32m    420\u001b[39m     sucesso, dados_brutos = pipeline_dim_colaboradores()\n\u001b[32m    422\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sucesso:\n",
      "\u001b[31mNameError\u001b[39m: name 'pipeline_dim_calendario' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text, exc as sqlalchemy_exc\n",
    "# --- [INÍCIO DAS IMPORTAÇÕES CORRIGIDAS] ---\n",
    "import sys\n",
    "import numpy as np \n",
    "import json \n",
    "from decimal import Decimal, InvalidOperation\n",
    "from sqlalchemy.types import String, Date, Numeric # Usado apenas para a API, mas mantido\n",
    "# --- [FIM DAS IMPORTAÇÕES CORRIGIDAS] ---\n",
    "\n",
    "\n",
    "# 1. CARREGAR VARIÁVEIS DE AMBIENTE\n",
    "# -----------------------------------\n",
    "print(\"Iniciando ETL...\")\n",
    "load_dotenv()\n",
    "\n",
    "# Carrega o Token da API\n",
    "API_TOKEN = os.getenv('SOLIDES_API_TOKEN')\n",
    "\n",
    "# Carrega os componentes do Banco\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASS = os.getenv('DB_PASS')\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_PORT = os.getenv('DB_PORT')\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "DB_SCHEMA = os.getenv('DB_SCHEMA')\n",
    "\n",
    "# Verifica se tudo foi carregado\n",
    "if not all([API_TOKEN, DB_USER, DB_PASS, DB_HOST, DB_PORT, DB_NAME, DB_SCHEMA]):\n",
    "    print(\"ERRO: Faltando uma ou mais variáveis no arquivo .env\")\n",
    "    print(f\"API_TOKEN Carregado: {'Sim' if API_TOKEN else 'NÃO'}\")\n",
    "    print(f\"DB_SCHEMA Carregado: {DB_SCHEMA}\")\n",
    "    sys.exit() # Encerra o script se faltar configuração\n",
    "\n",
    "# 2. CONFIGURAÇÕES GLOBAIS\n",
    "# -----------------------------------\n",
    "DB_URL = f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "BASE_URL = \"https://app.solides.com/pt-BR/api/v1\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Token token={API_TOKEN}\",\n",
    "    \"Accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "# 3. CRIA A CONEXÃO E GARANTE O SCHEMA (COM ASPAS)\n",
    "# ----------------------------------------------------\n",
    "try:\n",
    "    engine = create_engine(DB_URL)\n",
    "    with engine.begin() as conn:\n",
    "        \n",
    "        # 1. Garante que o schema (\"FOPAG\") existe PRIMEIRO.\n",
    "        conn.execute(text(f'CREATE SCHEMA IF NOT EXISTS \\\"{DB_SCHEMA}\\\"'))\n",
    "        \n",
    "        # 2. Instala a extensão explicitamente DENTRO do seu schema.\n",
    "        conn.execute(text(f'CREATE EXTENSION IF NOT EXISTS unaccent WITH SCHEMA \\\"{DB_SCHEMA}\\\";'))\n",
    "\n",
    "    # Recria a engine, definindo o search_path\n",
    "    engine = create_engine(\n",
    "        DB_URL,\n",
    "        connect_args={'options': f'-csearch_path=\\\"{DB_SCHEMA}\\\"'}\n",
    "    )\n",
    "\n",
    "    print(f\"Conexão com PostgreSQL estabelecida e schema '\\\"{DB_SCHEMA}\\\"' garantido.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao conectar ao PostgreSQL ou criar schema: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "# --- FUNÇÕES HELPER ---\n",
    "\n",
    "def limpar_salario_api(salario_str):\n",
    "    \"\"\"Limpa a string de salário vinda da API (ex: \"R$ 8.200,00\") para float.\"\"\"\n",
    "    if salario_str is None or pd.isna(salario_str):\n",
    "        return np.nan\n",
    "    try:\n",
    "        # Remove 'R$', espaços, e usa '.' como separador de milhar\n",
    "        salario_limpo = str(salario_str).replace('R$', '').replace(' ', '').replace('.', '')\n",
    "        # Troca ',' por '.' para ser decimal\n",
    "        salario_limpo = salario_limpo.replace(',', '.')\n",
    "        return pd.to_numeric(salario_limpo, errors='coerce')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# --- [INÍCIO DA ATUALIZAÇÃO] ---\n",
    "# A 'dim_colaboradores_base' agora é a dimensão MESTRE\n",
    "def atualizar_dim_colaboradores_base(engine, df_colaboradores, schema_name):\n",
    "    \"\"\"\n",
    "    Cria a tabela dim_colaboradores_base (se não existir) e\n",
    "    faz o UPSERT (INSERT ... ON CONFLICT) dos dados de colaboradores.\n",
    "    AGORA, esta tabela contém os dados mestres vindos do CSV.\n",
    "    \"\"\"\n",
    "    NOME_TABELA_BASE = \"dim_colaboradores_base\"\n",
    "    NOME_TABELA_STAGING_TEMP = \"stg_colab_temp_upsert\" \n",
    "\n",
    "    if df_colaboradores is None or df_colaboradores.empty:\n",
    "        print(\"Nenhum dado de colaborador fornecido para o UPSERT.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- Iniciando UPSERT para '{NOME_TABELA_BASE}' (Tabela Mestre) ---\")\n",
    "\n",
    "    # SQL para criar a tabela base (AGORA ENRIQUECIDA)\n",
    "    sql_create_base = text(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS \\\"{schema_name}\\\".\\\"{NOME_TABELA_BASE}\\\" (\n",
    "            colaborador_sk SERIAL PRIMARY KEY,\n",
    "            nome_colaborador VARCHAR(255) NOT NULL,\n",
    "            cpf VARCHAR(20) UNIQUE NOT NULL,\n",
    "            \n",
    "            -- Novos campos mestres (do CSV)\n",
    "            data_admissao_csv DATE,\n",
    "            data_demissao_csv DATE,\n",
    "            situacao_csv VARCHAR(100),\n",
    "            departamento_csv VARCHAR(255),\n",
    "            cargo_csv VARCHAR(255)\n",
    "        );\n",
    "        INSERT INTO \\\"{schema_name}\\\".\\\"{NOME_TABELA_BASE}\\\" (colaborador_sk, nome_colaborador, cpf)\n",
    "        VALUES (0, 'Desconhecido', 'N/A')\n",
    "        ON CONFLICT (colaborador_sk) DO NOTHING;\n",
    "    \"\"\")\n",
    "\n",
    "    # SQL de UPSERT (AGORA ENRIQUECIDO)\n",
    "    sql_upsert = text(f\"\"\"\n",
    "        INSERT INTO \\\"{schema_name}\\\".\\\"{NOME_TABELA_BASE}\\\" (\n",
    "            nome_colaborador, cpf, \n",
    "            data_admissao_csv, data_demissao_csv, situacao_csv, \n",
    "            departamento_csv, cargo_csv\n",
    "        )\n",
    "        SELECT\n",
    "            DISTINCT ON (src.cpf)\n",
    "            src.nome_colaborador,\n",
    "            src.cpf,\n",
    "            src.data_admissao_csv,\n",
    "            src.data_demissao_csv,\n",
    "            src.situacao_csv,\n",
    "            src.departamento_csv,\n",
    "            src.cargo_csv\n",
    "        FROM\n",
    "            \\\"{schema_name}\\\".\\\"{NOME_TABELA_STAGING_TEMP}\\\" AS src\n",
    "        WHERE\n",
    "            src.cpf IS NOT NULL AND src.cpf != 'N/A'\n",
    "        ORDER BY\n",
    "            src.cpf, src.nome_colaborador DESC\n",
    "        ON CONFLICT (cpf) DO UPDATE SET\n",
    "            nome_colaborador = EXCLUDED.nome_colaborador,\n",
    "            data_admissao_csv = EXCLUDED.data_admissao_csv,\n",
    "            data_demissao_csv = EXCLUDED.data_demissao_csv,\n",
    "            situacao_csv = EXCLUDED.situacao_csv,\n",
    "            departamento_csv = EXCLUDED.departamento_csv,\n",
    "            cargo_csv = EXCLUDED.cargo_csv;\n",
    "    \"\"\")\n",
    "    \n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            # 1. Cria a tabela base (se não existir) com a NOVA ESTRUTURA\n",
    "            conn.execute(sql_create_base)\n",
    "\n",
    "            # 2. Carga dos dados do DataFrame para a tabela temporária de staging\n",
    "            # O DataFrame já deve vir com os nomes de colunas corretos \n",
    "            # (ex: 'data_admissao_csv')\n",
    "            df_colaboradores.to_sql(\n",
    "                NOME_TABELA_STAGING_TEMP,\n",
    "                con=conn,\n",
    "                schema=schema_name,\n",
    "                if_exists='replace',\n",
    "                index=False\n",
    "            )\n",
    "\n",
    "            # 3. Executa o UPSERT (agora enriquecido)\n",
    "            conn.execute(sql_upsert)\n",
    "\n",
    "            # 4. (Opcional) Limpa a tabela temporária\n",
    "            conn.execute(text(f\"DROP TABLE \\\"{schema_name}\\\".\\\"{NOME_TABELA_STAGING_TEMP}\\\"\"))\n",
    "\n",
    "        print(f\"SUCESSO! '{NOME_TABELA_BASE}' (Mestre) foi atualizada com os dados do DataFrame.\")\n",
    "        # --- [INÍCIO DA CORREÇÃO 3/3 - Parte 1] ---\n",
    "        return True # <-- Retorna Sucesso\n",
    "        # --- [FIM DA CORREÇÃO 3/3 - Parte 1] ---\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERRO ao fazer UPSERT na '{NOME_TABELA_BASE}': {e}\")\n",
    "        # --- [INÍCIO DA CORREÇÃO 3/3 - Parte 2] ---\n",
    "        return False # <-- Retorna Falha\n",
    "        # --- [FIM DA CORREÇÃO 3/3 - Parte 2] ---\n",
    "# --- [FIM DA ATUALIZAÇÃO] ---\n",
    "\n",
    "\n",
    "# --- FASE 1: PIPELINES DAS DIMENSÕES (API) ---\n",
    "\n",
    "def pipeline_dim_colaboradores():\n",
    "    \"\"\"\n",
    "    PUXA dados de Colaboradores da API (paginado) e carrega na dim_colaboradores.\n",
    "    Popula a dim_colaboradores_base com API (será sobrescrito/enriquecido pelo CSV).\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Iniciando Pipeline: dim_colaboradores (API Sólides) ---\")\n",
    "\n",
    "    # 1. Extração (E)\n",
    "    # (Lógica de extração idêntica)\n",
    "    all_colaboradores_lista = []\n",
    "    page = 1\n",
    "    page_size = 100\n",
    "    ENDPOINT_LISTA = \"/colaboradores\" \n",
    "    print(\"Iniciando extração (Passo 1/2): Buscando lista de IDs de colaboradores...\")\n",
    "    while True:\n",
    "        params = {'page': page, 'page_size': page_size, 'status': 'todos'} \n",
    "        try:\n",
    "            response = requests.get(f\"{BASE_URL}{ENDPOINT_LISTA}\", headers=HEADERS, params=params)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if not data:\n",
    "                    print(f\"Extração da lista concluída. Total de {len(all_colaboradores_lista)} colaboradores encontrados.\")\n",
    "                    break\n",
    "                all_colaboradores_lista.extend(data) \n",
    "                print(f\"Página {page} da lista carregada...\")\n",
    "                page += 1\n",
    "            else:\n",
    "                print(f\"Erro na API (Página {page}): {response.status_code} {response.text}\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na extração de colaboradores (lista): {e}\")\n",
    "            return False\n",
    "    if not all_colaboradores_lista:\n",
    "        print(\"Nenhum colaborador encontrado.\")\n",
    "        return True\n",
    "    print(f\"Passo 1/2 concluído. {len(all_colaboradores_lista)} colaboradores encontrados.\")\n",
    "    all_colaboradores_detalhado = []\n",
    "    total_colabs = len(all_colaboradores_lista)\n",
    "    print(f\"Iniciando extração (Passo 2/2): Buscando detalhes completos...\")\n",
    "    for i, colab_info in enumerate(all_colaboradores_lista):\n",
    "        colab_id = colab_info.get('id')\n",
    "        if not colab_id:\n",
    "            continue\n",
    "        print(f\"   Buscando colaborador {i+1} de {total_colabs} (ID: {colab_id})...\")\n",
    "        ENDPOINT_DETALHE = f\"/colaboradores/{colab_id}\"\n",
    "        try:\n",
    "            response_detalhe = requests.get(f\"{BASE_URL}{ENDPOINT_DETALHE}\", headers=HEADERS)\n",
    "            if response_detalhe.status_code == 200:\n",
    "                data_detalhe = response_detalhe.json()\n",
    "                all_colaboradores_detalhado.append(data_detalhe)\n",
    "            else:\n",
    "                print(f\"    ERRO ao buscar detalhes do ID {colab_id}: {response_detalhe.status_code}. Usando dados básicos da lista.\")\n",
    "                all_colaboradores_detalhado.append(colab_info) \n",
    "        except Exception as e:\n",
    "            print(f\"    EXCEÇÃO ao buscar detalhes do ID {colab_id}: {e}. Usando dados básicos da lista.\")\n",
    "            all_colaboradores_detalhado.append(colab_info) \n",
    "    print(\"Passo 2/2 concluído. Detalhes de todos os colaboradores buscados.\")\n",
    "\n",
    "    # 2. Transformação (T)\n",
    "    df = pd.json_normalize(all_colaboradores_detalhado)\n",
    "\n",
    "    # --- (Lógica de transformação da API Sólides - IDÊNTICA A ANTES) ---\n",
    "    df['dept_name_temp'] = None\n",
    "    if 'departament.name' in df.columns:\n",
    "        print(\"Info: Departamento encontrado na chave 'departament.name'.\")\n",
    "        df['dept_name_temp'] = df['departament.name']\n",
    "    elif 'department.name' in df.columns: \n",
    "        print(\"Info: Departamento encontrado na chave 'department.name'.\")\n",
    "        df['dept_name_temp'] = df['department.name']\n",
    "    else:\n",
    "        print(\"Aviso: Nenhuma chave de Departamento ('departament.name', 'department.name') foi encontrada.\")\n",
    "    df['cargo_name_temp'] = None\n",
    "    if 'position.name' in df.columns:\n",
    "        print(\"Info: Cargo encontrado na chave 'position.name'.\")\n",
    "        df['cargo_name_temp'] = df['position.name']\n",
    "    elif 'cargo.name' in df.columns: \n",
    "        print(\"Info: Cargo encontrado na chave 'cargo.name'.\")\n",
    "        df['cargo_name_temp'] = df['cargo.name']\n",
    "    else:\n",
    "        print(\"Aviso: Nenhuma chave de Cargo ('position.name', 'cargo.name') foi encontrada.\")\n",
    "    df['education_level_temp'] = None\n",
    "    if 'education' in df.columns: \n",
    "        print(\"Info: Nível Educacional encontrado na chave 'education'.\")\n",
    "        df['education_level_temp'] = df['education']\n",
    "    elif 'educationLevel' in df.columns: \n",
    "        print(\"Info: Nível Educacional encontrado na chave 'educationLevel'.\")\n",
    "        df['education_level_temp'] = df['educationLevel']\n",
    "    elif 'scholarship' in df.columns: \n",
    "        print(\"Info: Nível Educacional encontrado na chave 'scholarship'.\")\n",
    "        df['education_level_temp'] = df['scholarship']\n",
    "    elif 'schooling' in df.columns:\n",
    "        print(\"Info: Nível Educacional encontrado na chave 'schooling'.\")\n",
    "        df['education_level_temp'] = df['schooling']\n",
    "    elif 'escolaridade' in df.columns: \n",
    "        print(\"Info: Nível Educacional encontrado na chave 'escolaridade'.\")\n",
    "        df['education_level_temp'] = df['escolaridade']\n",
    "    else:\n",
    "        print(\"Aviso: Nenhuma chave de Nível Educacional ('education', 'educationLevel', 'scholarship', 'schooling', 'escolaridade') foi encontrada.\")\n",
    "    df['cpf_temp'] = None \n",
    "    if 'documents.idNumber' in df.columns: \n",
    "        print(\"Info: CPF encontrado na chave 'documents.idNumber'.\")\n",
    "        df['cpf_temp'] = df['documents.idNumber']\n",
    "    elif 'documents.cpf' in df.columns:\n",
    "        print(\"Info: CPF encontrado na chave 'documents.cpf'.\")\n",
    "        df['cpf_temp'] = df['documents.cpf']\n",
    "    elif 'idNumber' in df.columns:\n",
    "        print(\"Info: CPF encontrado na chave 'idNumber' (raiz).\")\n",
    "        df['cpf_temp'] = df['idNumber']\n",
    "    elif 'cpf' in df.columns:\n",
    "        print(\"Info: CPF encontrado na chave 'cpf' (raiz).\")\n",
    "        df['cpf_temp'] = df['cpf']\n",
    "    elif 'document' in df.columns:\n",
    "        print(\"Info: CPF encontrado na chave 'document' (raiz).\")\n",
    "        df['cpf_temp'] = df['document']\n",
    "    else:\n",
    "        print(\"Aviso: Nenhuma chave de CPF ('documents.idNumber', 'idNumber', 'cpf', 'document') foi encontrada.\")\n",
    "    if 'cpf_temp' in df.columns:\n",
    "         df['cpf_temp'] = df['cpf_temp'].astype(str).str.replace(r'\\D', '', regex=True)\n",
    "         df['cpf_temp'] = df['cpf_temp'].replace(r'^\\s*$', np.nan, regex=True).replace('None', np.nan).replace('nan', np.nan)\n",
    "    else:\n",
    "         df['cpf_temp'] = None\n",
    "    if 'salary' in df.columns:\n",
    "        df['salario_api_temp'] = df['salary'].apply(limpar_salario_api)\n",
    "    else:\n",
    "        df['salario_api_temp'] = np.nan\n",
    "        \n",
    "    # --- (Dicionário de Renomeação - IDÊNTICO) ---\n",
    "    df = df.rename(columns={\n",
    "        'id': 'colaborador_id_solides',\n",
    "        'name': 'nome_completo',\n",
    "        'cpf_temp': 'cpf', \n",
    "        'birthDate': 'data_nascimento',\n",
    "        'gender': 'genero',\n",
    "        'dateAdmission': 'data_admissao',\n",
    "        'dateDismissal': 'data_demissao',\n",
    "        'active': 'ativo',\n",
    "        'dept_name_temp': 'departamento_nome_api', \n",
    "        'cargo_name_temp': 'cargo_nome_api',           \n",
    "        'email': 'email', # Email principal (geralmente corporativo)\n",
    "        'contact.phone': 'telefone_pessoal', \n",
    "        'contact.cellPhone': 'celular', \n",
    "        'nationality': 'nacionalidade',\n",
    "        'education_level_temp': 'nivel_educacional', \n",
    "        'motherName': 'nome_mae',\n",
    "        'fatherName': 'nome_pai',\n",
    "        'address.streetName': 'logradouro', \n",
    "        'address.number': 'numero_endereco',\n",
    "        'address.additionalInformation': 'complemento_endereco', \n",
    "        'address.neighborhood': 'bairro',\n",
    "        'address.city.name': 'cidade', \n",
    "        'address.state.initials': 'estado', \n",
    "        'address.zipCode': 'cep',\n",
    "        'registration': 'matricula',\n",
    "        'maritalStatus': 'estado_civil',\n",
    "        'salario_api_temp': 'salario_api',\n",
    "        'workShift': 'turno_trabalho',\n",
    "        'typeContract': 'tipo_contrato',\n",
    "        'course': 'curso_formacao',\n",
    "        'hierarchicalLevel': 'nivel_hierarquico',\n",
    "        'senior.name': 'nome_lider_imediato',\n",
    "        'ethnicity': 'etnia',\n",
    "        'unity.name': 'unidade_nome',\n",
    "        'salutation': 'saudacao',\n",
    "        'typeOfSpecialNeed': 'tipo_necessidade_especial',\n",
    "        'birthplace': 'local_nascimento',\n",
    "        'disabledPerson': 'pcd',\n",
    "        'reasonDismissal': 'motivo_demissao_api',\n",
    "        'dateContract': 'data_contrato',\n",
    "        'durationContract': 'duracao_contrato',\n",
    "        'contractExpirationDate': 'data_expiracao_contrato',\n",
    "        'experiencePeriod': 'periodo_experiencia_dias',\n",
    "        'formDismissal': 'forma_demissao',\n",
    "        'decisionDismissal': 'decisao_demissao',\n",
    "        'terminationAmount': 'valor_rescisao',\n",
    "        'totalBenefits': 'total_beneficios_api',\n",
    "        'updated_at': 'data_ultima_atualizacao_api',\n",
    "        'contact.emergencyPhoneNumber': 'telefone_emergencia',\n",
    "        'contact.personalEmail': 'email_pessoal',\n",
    "        'contact.corporateEmail': 'email_corporativo_sec', # 'email' raiz já foi pego\n",
    "        'position.id': 'cargo_id_solides',\n",
    "        'departament.id': 'departamento_id_solides',\n",
    "        'documents.rg': 'rg',\n",
    "        'documents.dispatchDate': 'data_emissao_rg',\n",
    "        'documents.issuingBody': 'orgao_emissor_rg',\n",
    "        'documents.voterRegistration': 'titulo_eleitor',\n",
    "        'documents.electoralZone': 'zona_eleitoral',\n",
    "        'documents.electoralSection': 'secao_eleitoral',\n",
    "        'documents.ctpsNum': 'ctps_numero',\n",
    "        'documents.ctpsSerie': 'ctps_serie',\n",
    "        'documents.pis': 'pis'\n",
    "    })\n",
    "    \n",
    "    # --- (Tratamento de Tipos - IDÊNTICO) ---\n",
    "    date_cols = [\n",
    "        'data_nascimento', 'data_admissao', 'data_demissao',\n",
    "        'data_contrato', 'data_expiracao_contrato', 'data_emissao_rg', \n",
    "        'data_ultima_atualizacao_api'\n",
    "    ]\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col], format='%d/%m/%Y', errors='raise')\n",
    "            except ValueError:\n",
    "                df[col] = pd.to_datetime(df[col], format='%Y-%m-%d', errors='coerce')\n",
    "        else:\n",
    "             df[col] = pd.NaT\n",
    "    numeric_cols = [\n",
    "        'valor_rescisao', 'total_beneficios_api', 'periodo_experiencia_dias',\n",
    "        'cargo_id_solides', 'departamento_id_solides'\n",
    "    ]\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    boolean_cols = ['pcd'] # 'disabledPerson'\n",
    "    for col in boolean_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype('boolean')\n",
    "            \n",
    "    # --- (Lista de Staging - IDÊNTICA) ---\n",
    "    colunas_staging = [\n",
    "        'colaborador_id_solides', 'cpf', 'nome_completo', 'data_nascimento', 'genero',\n",
    "        'data_admissao', 'data_demissao', 'ativo',\n",
    "        'departamento_nome_api', 'cargo_nome_api',\n",
    "        'email', 'telefone_pessoal', 'celular', 'nacionalidade', 'nivel_educacional',\n",
    "        'nome_mae', 'nome_pai',\n",
    "        'logradouro', 'numero_endereco', 'complemento_endereco', 'bairro', 'cidade', 'estado', 'cep',\n",
    "        'matricula', 'estado_civil', 'salario_api', 'turno_trabalho', 'tipo_contrato',\n",
    "        'curso_formacao', 'nivel_hierarquico', 'nome_lider_imediato', 'etnia', 'unidade_nome',\n",
    "        'saudacao', 'tipo_necessidade_especial', 'local_nascimento', 'pcd', \n",
    "        'motivo_demissao_api', 'data_contrato', 'duracao_contrato', \n",
    "        'data_expiracao_contrato', 'periodo_experiencia_dias', 'forma_demissao',\n",
    "        'decisao_demissao', 'valor_rescisao', 'total_beneficios_api', \n",
    "        'data_ultima_atualizacao_api', 'telefone_emergencia', 'email_pessoal',\n",
    "        'email_corporativo_sec', 'cargo_id_solides', 'departamento_id_solides',\n",
    "        'rg', 'data_emissao_rg', 'orgao_emissor_rg', 'titulo_eleitor', \n",
    "        'zona_eleitoral', 'secao_eleitoral', 'ctps_numero', 'ctps_serie', 'pis'\n",
    "    ]\n",
    "    \n",
    "    for col in colunas_staging:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None \n",
    "    df_staging = df[colunas_staging].copy()\n",
    "    print(\"Transformação de colaboradores concluída.\")\n",
    "\n",
    "    # 3. Carga (L)\n",
    "    NOME_TABELA_RICA = \"dim_colaboradores\" # Tabela rica da API\n",
    "    NOME_TABELA_BASE = \"dim_colaboradores_base\" # Tabela conformada (SK, CPF, Nome)\n",
    "    NOME_TABELA_STAGING = \"staging_colaboradores\"\n",
    "\n",
    "    try:\n",
    "        df_staging.to_sql(NOME_TABELA_STAGING, engine, if_exists='replace', index=False, schema=DB_SCHEMA)\n",
    "        print(\"Carga na staging de colaboradores concluída.\")\n",
    "\n",
    "        # --- [INÍCIO DA CORREÇÃO 1/3] ---\n",
    "        # Expansão do SQL de Carga (Adicionando ALTER TABLE)\n",
    "        sql = f\"\"\"\n",
    "        -- PASSO 1: Cria a Tabela Base (Conformada) conforme a NOVA ESTRUTURA MESTRE\n",
    "        CREATE TABLE IF NOT EXISTS \"{DB_SCHEMA}\".{NOME_TABELA_BASE} (\n",
    "            colaborador_sk SERIAL PRIMARY KEY,\n",
    "            nome_colaborador VARCHAR(255) NOT NULL,\n",
    "            cpf VARCHAR(20) UNIQUE NOT NULL,\n",
    "            \n",
    "            -- Campos mestres (do CSV)\n",
    "            data_admissao_csv DATE,\n",
    "            data_demissao_csv DATE,\n",
    "            situacao_csv VARCHAR(100),\n",
    "            departamento_csv VARCHAR(255),\n",
    "            cargo_csv VARCHAR(255)\n",
    "        );\n",
    "        INSERT INTO \"{DB_SCHEMA}\".{NOME_TABELA_BASE} (colaborador_sk, nome_colaborador, cpf)\n",
    "        VALUES (0, 'Desconhecido', 'N/A')\n",
    "        ON CONFLICT (colaborador_sk) DO NOTHING;\n",
    "\n",
    "        -- *** CORREÇÃO PARA ERRO 1 ***\n",
    "        -- Garante que as colunas mestre existem, mesmo se a tabela foi criada por um script antigo\n",
    "        ALTER TABLE \"{DB_SCHEMA}\".{NOME_TABELA_BASE}\n",
    "            ADD COLUMN IF NOT EXISTS data_admissao_csv DATE,\n",
    "            ADD COLUMN IF NOT EXISTS data_demissao_csv DATE,\n",
    "            ADD COLUMN IF NOT EXISTS situacao_csv VARCHAR(100),\n",
    "            ADD COLUMN IF NOT EXISTS departamento_csv VARCHAR(255),\n",
    "            ADD COLUMN IF NOT EXISTS cargo_csv VARCHAR(255);\n",
    "        -- *** FIM DA CORREÇÃO 1/3 ***\n",
    "\n",
    "        -- PASSO 2: Faz o UPSERT da API (staging) para a Tabela Base\n",
    "        -- Nota: Isso só popula Nome e CPF. O CSV irá enriquecer o restante.\n",
    "        INSERT INTO \"{DB_SCHEMA}\".{NOME_TABELA_BASE} (nome_colaborador, cpf)\n",
    "        SELECT\n",
    "            DISTINCT ON (stg.cpf)\n",
    "            stg.nome_completo,\n",
    "            stg.cpf\n",
    "        FROM\n",
    "            \"{DB_SCHEMA}\".{NOME_TABELA_STAGING} AS stg\n",
    "        WHERE\n",
    "            stg.cpf IS NOT NULL AND stg.cpf != 'N/A'\n",
    "        ORDER BY\n",
    "            stg.cpf, stg.colaborador_id_solides DESC \n",
    "        ON CONFLICT (cpf) DO UPDATE SET\n",
    "            nome_colaborador = EXCLUDED.nome_colaborador;\n",
    "\n",
    "        -- PASSO 3: Cria a Tabela Rica (dim_colaboradores)\n",
    "        -- (SQL IDÊNTICO AO ANTERIOR)\n",
    "        CREATE TABLE IF NOT EXISTS \"{DB_SCHEMA}\".{NOME_TABELA_RICA} (\n",
    "            colaborador_sk INTEGER PRIMARY KEY, \n",
    "            colaborador_id_solides INTEGER UNIQUE NOT NULL, \n",
    "            cpf VARCHAR(11), \n",
    "            nome_completo VARCHAR(255),\n",
    "            data_nascimento DATE,\n",
    "            genero VARCHAR(50),\n",
    "            data_admissao DATE,\n",
    "            data_demissao DATE,\n",
    "            ativo BOOLEAN,\n",
    "            departamento_nome_api VARCHAR(255),\n",
    "            cargo_nome_api VARCHAR(255),\n",
    "            email VARCHAR(255),\n",
    "            telefone_pessoal VARCHAR(50),\n",
    "            celular VARCHAR(50),\n",
    "            nacionalidade VARCHAR(100),\n",
    "            nivel_educacional VARCHAR(100),\n",
    "            nome_mae VARCHAR(255),\n",
    "            nome_pai VARCHAR(255),\n",
    "            logradouro VARCHAR(255),\n",
    "            numero_endereco VARCHAR(50),\n",
    "            complemento_endereco VARCHAR(100),\n",
    "            bairro VARCHAR(100),\n",
    "            cidade VARCHAR(100),\n",
    "            estado VARCHAR(50),\n",
    "            cep VARCHAR(20),\n",
    "            matricula VARCHAR(50),\n",
    "            estado_civil VARCHAR(50),\n",
    "            salario_api NUMERIC(12, 2),\n",
    "            turno_trabalho VARCHAR(100),\n",
    "            tipo_contrato VARCHAR(100),\n",
    "            curso_formacao VARCHAR(255),\n",
    "            nivel_hierarquico VARCHAR(100),\n",
    "            nome_lider_imediato VARCHAR(255),\n",
    "            etnia VARCHAR(50),\n",
    "            unidade_nome VARCHAR(255),\n",
    "            data_ultima_atualizacao TIMESTAMP DEFAULT current_timestamp,\n",
    "            FOREIGN KEY (colaborador_sk) REFERENCES \"{DB_SCHEMA}\".{NOME_TABELA_BASE}(colaborador_sk)\n",
    "        );\n",
    "\n",
    "        -- (SQL IDÊNTICO AO ANTERIOR)\n",
    "        ALTER TABLE \"{DB_SCHEMA}\".{NOME_TABELA_RICA}\n",
    "            ADD COLUMN IF NOT EXISTS matricula VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS estado_civil VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS salario_api NUMERIC(12, 2),\n",
    "            ADD COLUMN IF NOT EXISTS turno_trabalho VARCHAR(100),\n",
    "            ADD COLUMN IF NOT EXISTS tipo_contrato VARCHAR(100),\n",
    "            ADD COLUMN IF NOT EXISTS curso_formacao VARCHAR(255),\n",
    "            ADD COLUMN IF NOT EXISTS nivel_hierarquico VARCHAR(100),\n",
    "            ADD COLUMN IF NOT EXISTS nome_lider_imediato VARCHAR(255),\n",
    "            ADD COLUMN IF NOT EXISTS etnia VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS unidade_nome VARCHAR(255),\n",
    "            ADD COLUMN IF NOT EXISTS saudacao VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS tipo_necessidade_especial VARCHAR(100),\n",
    "            ADD COLUMN IF NOT EXISTS local_nascimento VARCHAR(100),\n",
    "            ADD COLUMN IF NOT EXISTS pcd BOOLEAN,\n",
    "            ADD COLUMN IF NOT EXISTS motivo_demissao_api VARCHAR(255),\n",
    "            ADD COLUMN IF NOT EXISTS data_contrato DATE,\n",
    "            ADD COLUMN IF NOT EXISTS duracao_contrato VARCHAR(100),\n",
    "            ADD COLUMN IF NOT EXISTS data_expiracao_contrato DATE,\n",
    "            ADD COLUMN IF NOT EXISTS periodo_experiencia_dias INTEGER,\n",
    "            ADD COLUMN IF NOT EXISTS forma_demissao VARCHAR(100),\n",
    "            ADD COLUMN IF NOT EXISTS decisao_demissao VARCHAR(100),\n",
    "            ADD COLUMN IF NOT EXISTS valor_rescisao NUMERIC(12, 2),\n",
    "            ADD COLUMN IF NOT EXISTS total_beneficios_api NUMERIC(12, 2),\n",
    "            ADD COLUMN IF NOT EXISTS data_ultima_atualizacao_api DATE,\n",
    "            ADD COLUMN IF NOT EXISTS telefone_emergencia VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS email_pessoal VARCHAR(255),\n",
    "            ADD COLUMN IF NOT EXISTS email_corporativo_sec VARCHAR(255),\n",
    "            ADD COLUMN IF NOT EXISTS cargo_id_solides INTEGER,\n",
    "            ADD COLUMN IF NOT EXISTS departamento_id_solides INTEGER,\n",
    "            ADD COLUMN IF NOT EXISTS rg VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS data_emissao_rg DATE,\n",
    "            ADD COLUMN IF NOT EXISTS orgao_emissor_rg VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS titulo_eleitor VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS zona_eleitoral VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS secao_eleitoral VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS ctps_numero VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS ctps_serie VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS pis VARCHAR(50);\n",
    "        \n",
    "        -- (SQL IDÊNTICO AO ANTERIOR)\n",
    "        INSERT INTO \"{DB_SCHEMA}\".{NOME_TABELA_RICA} (colaborador_sk, colaborador_id_solides)\n",
    "        VALUES (0, -1)\n",
    "        ON CONFLICT (colaborador_sk) DO NOTHING;\n",
    "\n",
    "        -- PASSO 4: Faz o UPSERT na Tabela Rica (com todas as colunas)\n",
    "        -- (SQL IDÊNTICO AO ANTERIOR)\n",
    "        INSERT INTO \"{DB_SCHEMA}\".{NOME_TABELA_RICA} (\n",
    "            colaborador_sk, \n",
    "            colaborador_id_solides, \n",
    "            cpf, \n",
    "            nome_completo, data_nascimento, genero,\n",
    "            nacionalidade, nivel_educacional, nome_mae, nome_pai,\n",
    "            estado_civil, etnia,\n",
    "            data_admissao, data_demissao, ativo,\n",
    "            departamento_nome_api, cargo_nome_api,\n",
    "            matricula, salario_api, turno_trabalho, tipo_contrato,\n",
    "            curso_formacao, nivel_hierarquico, nome_lider_imediato,\n",
    "            unidade_nome,\n",
    "            email, telefone_pessoal, celular, \n",
    "            logradouro, numero_endereco, complemento_endereco, bairro, \n",
    "            cidade, estado, cep,\n",
    "            saudacao, tipo_necessidade_especial, local_nascimento, pcd,\n",
    "            telefone_emergencia, email_pessoal, email_corporativo_sec,\n",
    "            rg, data_emissao_rg, orgao_emissor_rg, titulo_eleitor,\n",
    "            zona_eleitoral, secao_eleitoral, ctps_numero, ctps_serie, pis,\n",
    "            motivo_demissao_api, data_contrato, duracao_contrato,\n",
    "            data_expiracao_contrato, periodo_experiencia_dias, forma_demissao,\n",
    "            decisao_demissao, valor_rescisao, total_beneficios_api,\n",
    "            cargo_id_solides, departamento_id_solides,\n",
    "            data_ultima_atualizacao_api,\n",
    "            data_ultima_atualizacao\n",
    "        )\n",
    "        SELECT\n",
    "            base.colaborador_sk, \n",
    "            stg.colaborador_id_solides, \n",
    "            stg.cpf,\n",
    "            stg.nome_completo, stg.data_nascimento, stg.genero,\n",
    "            stg.nacionalidade, stg.nivel_educacional, stg.nome_mae, stg.nome_pai,\n",
    "            stg.estado_civil, stg.etnia,\n",
    "            stg.data_admissao, stg.data_demissao, stg.ativo,\n",
    "            stg.departamento_nome_api,\n",
    "            stg.cargo_nome_api,\n",
    "            stg.matricula, stg.salario_api, stg.turno_trabalho, stg.tipo_contrato,\n",
    "            stg.curso_formacao, stg.nivel_hierarquico, stg.nome_lider_imediato,\n",
    "            stg.unidade_nome,\n",
    "            stg.email, stg.telefone_pessoal, stg.celular,\n",
    "            stg.logradouro, stg.numero_endereco, stg.complemento_endereco, stg.bairro, \n",
    "            stg.cidade, stg.estado, stg.cep,\n",
    "            stg.saudacao, stg.tipo_necessidade_especial, stg.local_nascimento, stg.pcd,\n",
    "            stg.telefone_emergencia, stg.email_pessoal, stg.email_corporativo_sec,\n",
    "            stg.rg, stg.data_emissao_rg, stg.orgao_emissor_rg, stg.titulo_eleitor,\n",
    "            stg.zona_eleitoral, stg.secao_eleitoral, stg.ctps_numero, stg.ctps_serie, stg.pis,\n",
    "            stg.motivo_demissao_api, stg.data_contrato, stg.duracao_contrato,\n",
    "            stg.data_expiracao_contrato, stg.periodo_experiencia_dias, stg.forma_demissao,\n",
    "            stg.decisao_demissao, stg.valor_rescisao, stg.total_beneficios_api,\n",
    "            stg.cargo_id_solides, stg.departamento_id_solides,\n",
    "            stg.data_ultima_atualizacao_api,\n",
    "            current_timestamp\n",
    "        FROM\n",
    "            \"{DB_SCHEMA}\".{NOME_TABELA_STAGING} AS stg\n",
    "        JOIN\n",
    "            \"{DB_SCHEMA}\".{NOME_TABELA_BASE} AS base ON stg.cpf = base.cpf\n",
    "        WHERE\n",
    "            stg.colaborador_id_solides IS NOT NULL\n",
    "\n",
    "        ON CONFLICT (colaborador_id_solides) DO UPDATE SET\n",
    "            cpf = EXCLUDED.cpf, \n",
    "            nome_completo = EXCLUDED.nome_completo,\n",
    "            data_nascimento = EXCLUDED.data_nascimento,\n",
    "            genero = EXCLUDED.genero,\n",
    "            nacionalidade = EXCLUDED.nacionalidade,\n",
    "            nivel_educacional = EXCLUDED.nivel_educacional,\n",
    "            nome_mae = EXCLUDED.nome_mae,\n",
    "            nome_pai = EXCLUDED.nome_pai,\n",
    "            estado_civil = EXCLUDED.estado_civil,\n",
    "            etnia = EXCLUDED.etnia,\n",
    "            data_admissao = EXCLUDED.data_admissao,\n",
    "            data_demissao = EXCLUDED.data_demissao,\n",
    "            ativo = EXCLUDED.ativo,\n",
    "            departamento_nome_api = EXCLUDED.departamento_nome_api,\n",
    "            cargo_nome_api = EXCLUDED.cargo_nome_api,\n",
    "            matricula = EXCLUDED.matricula,\n",
    "            salario_api = EXCLUDED.salario_api,\n",
    "            turno_trabalho = EXCLUDED.turno_trabalho,\n",
    "            tipo_contrato = EXCLUDED.tipo_contrato,\n",
    "            curso_formacao = EXCLUDED.curso_formacao,\n",
    "            nivel_hierarquico = EXCLUDED.nivel_hierarquico,\n",
    "            nome_lider_imediato = EXCLUDED.nome_lider_imediato,\n",
    "            unidade_nome = EXCLUDED.unidade_nome,\n",
    "            email = EXCLUDED.email,\n",
    "            telefone_pessoal = EXCLUDED.telefone_pessoal,\n",
    "            celular = EXCLUDED.celular,\n",
    "            logradouro = EXCLUDED.logradouro,\n",
    "            numero_endereco = EXCLUDED.numero_endereco,\n",
    "            complemento_endereco = EXCLUDED.complemento_endereco,\n",
    "            bairro = EXCLUDED.bairro,\n",
    "            cidade = EXCLUDED.cidade,\n",
    "            estado = EXCLUDED.estado,\n",
    "            cep = EXCLUDED.cep,\n",
    "            saudacao = EXCLUDED.saudacao,\n",
    "            tipo_necessidade_especial = EXCLUDED.tipo_necessidade_especial,\n",
    "            local_nascimento = EXCLUDED.local_nascimento,\n",
    "            pcd = EXCLUDED.pcd,\n",
    "            telefone_emergencia = EXCLUDED.telefone_emergencia,\n",
    "            email_pessoal = EXCLUDED.email_pessoal,\n",
    "            email_corporativo_sec = EXCLUDED.email_corporativo_sec,\n",
    "            rg = EXCLUDED.rg,\n",
    "            data_emissao_rg = EXCLUDED.data_emissao_rg,\n",
    "            orgao_emissor_rg = EXCLUDED.orgao_emissor_rg,\n",
    "            titulo_eleitor = EXCLUDED.titulo_eleitor,\n",
    "            zona_eleitoral = EXCLUDED.zona_eleitoral,\n",
    "            secao_eleitoral = EXCLUDED.secao_eleitoral,\n",
    "            ctps_numero = EXCLUDED.ctps_numero,\n",
    "            ctps_serie = EXCLUDED.ctps_serie,\n",
    "            pis = EXCLUDED.pis,\n",
    "            motivo_demissao_api = EXCLUDED.motivo_demissao_api,\n",
    "            data_contrato = EXCLUDED.data_contrato,\n",
    "            duracao_contrato = EXCLUDED.duracao_contrato,\n",
    "            data_expiracao_contrato = EXCLUDED.data_expiracao_contrato,\n",
    "            periodo_experiencia_dias = EXCLUDED.periodo_experiencia_dias,\n",
    "            forma_demissao = EXCLUDED.forma_demissao,\n",
    "            decisao_demissao = EXCLUDED.decisao_demissao,\n",
    "            valor_rescisao = EXCLUDED.valor_rescisao,\n",
    "            total_beneficios_api = EXCLUDED.total_beneficios_api,\n",
    "            cargo_id_solides = EXCLUDED.cargo_id_solides,\n",
    "            departamento_id_solides = EXCLUDED.departamento_id_solides,\n",
    "            data_ultima_atualizacao_api = EXCLUDED.data_ultima_atualizacao_api,\n",
    "            data_ultima_atualizacao = current_timestamp;\n",
    "        \"\"\"\n",
    "        # --- [FIM DA ATUALIZAÇÃO] ---\n",
    "\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(sql))\n",
    "\n",
    "        print(f\"Carga na {NOME_TABELA_BASE} e {NOME_TABELA_RICA} concluída com sucesso!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro na carga de {NOME_TABELA_RICA}: {e}\")\n",
    "        print(f\"Detalhe do erro: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# --- NOVA DIMENSÃO: CALENDÁRIO ---\n",
    "def pipeline_dim_calendario():\n",
    "    \"\"\"Gera ou atualiza a dimensão de calendário (dim_calendario).\"\"\"\n",
    "    print(\"\\n--- Iniciando Pipeline: dim_calendario ---\")\n",
    "    \n",
    "    NOME_TABELA_FINAL = \"dim_calendario\"\n",
    "    \n",
    "    # (Lógica idêntica, sem alterações)\n",
    "    sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS \"{DB_SCHEMA}\".{NOME_TABELA_FINAL} (\n",
    "        data DATE PRIMARY KEY,\n",
    "        ano INTEGER,\n",
    "        mes INTEGER,\n",
    "        dia INTEGER,\n",
    "        trimestre INTEGER,\n",
    "        semestre INTEGER,\n",
    "        dia_da_semana INTEGER, \n",
    "        nome_dia_da_semana VARCHAR(20),\n",
    "        nome_mes VARCHAR(20),\n",
    "        nome_mes_abrev CHAR(3),\n",
    "        ano_mes VARCHAR(7), \n",
    "        dia_do_ano INTEGER,\n",
    "        semana_do_ano INTEGER\n",
    "    );\n",
    "    DO $$\n",
    "    DECLARE\n",
    "        data_inicio DATE := '2023-01-01'; \n",
    "        data_fim DATE := '2030-12-31';\n",
    "    BEGIN\n",
    "        BEGIN\n",
    "            SET LOCAL lc_time = 'pt_BR.UTF-8';\n",
    "        EXCEPTION WHEN OTHERS THEN\n",
    "            BEGIN\n",
    "                SET LOCAL lc_time = 'pt_BR';\n",
    "            EXCEPTION WHEN OTHERS THEN\n",
    "                RAISE NOTICE 'Não foi possível definir o locale pt_BR. Nomes de mês/dia podem ficar em inglês.';\n",
    "            END;\n",
    "        END;\n",
    "        INSERT INTO \"{DB_SCHEMA}\".{NOME_TABELA_FINAL} (\n",
    "            data,\n",
    "            ano, mes, dia, trimestre, semestre,\n",
    "            dia_da_semana, nome_dia_da_semana, nome_mes, nome_mes_abrev,\n",
    "            ano_mes, dia_do_ano, semana_do_ano\n",
    "        )\n",
    "        SELECT\n",
    "            d AS data,\n",
    "            EXTRACT(YEAR FROM d) AS ano,\n",
    "            EXTRACT(MONTH FROM d) AS mes,\n",
    "            EXTRACT(DAY FROM d) AS dia,\n",
    "            EXTRACT(QUARTER FROM d) AS trimestre,\n",
    "            CASE WHEN EXTRACT(MONTH FROM d) <= 6 THEN 1 ELSE 2 END AS semestre,\n",
    "            EXTRACT(DOW FROM d) AS dia_da_semana, \n",
    "            to_char(d, 'TMDay') AS nome_dia_da_semana,\n",
    "            to_char(d, 'TMMonth') AS nome_mes,\n",
    "            to_char(d, 'TMMon') AS nome_mes_abrev,\n",
    "            to_char(d, 'YYYY-MM') AS ano_mes,\n",
    "            EXTRACT(DOY FROM d) AS dia_do_ano,\n",
    "            EXTRACT(WEEK FROM d) AS semana_do_ano\n",
    "        FROM generate_series(data_inicio, data_fim, '1 day'::interval) d\n",
    "        ON CONFLICT (data) DO NOTHING; \n",
    "    END $$;\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(sql))\n",
    "        print(f\"Carga na {NOME_TABELA_FINAL} concluída com sucesso!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Erro na carga de {NOME_TABELA_FINAL}: {e}\")\n",
    "        print(f\"Detalhe do erro: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# --- FASE 2: PIPELINES DAS FATOS (CSV) ---\n",
    "\n",
    "# --- (FUNÇÕES HELPER DE CSV - IDÊNTICAS) ---\n",
    "\n",
    "def clean_text(series):\n",
    "    \"\"\"Limpa uma série de texto (object) de forma segura.\"\"\"\n",
    "    if series.dtype == 'object':\n",
    "        series = series.str.strip()\n",
    "        series = series.str.replace(u'\\xa0', '', regex=False)\n",
    "        series = series.replace(['N/A', '', 'nan', 'None', 'NULL'], None) # Adicionado 'NULL'\n",
    "    return series\n",
    "\n",
    "def para_float(valor_str):\n",
    "    \"\"\"Converte uma string (já limpa) para float.\"\"\"\n",
    "    if valor_str is None or pd.isna(valor_str):\n",
    "        return np.nan # Use numpy's NaN para floats\n",
    "    try:\n",
    "        # CSV do Notebook 1 salva com PONTO decimal (ex: \"1234.56\")\n",
    "        return float(valor_str) \n",
    "    except (ValueError, TypeError):\n",
    "        return np.nan\n",
    "\n",
    "def tratar_tipos_dataframe_csv(df, nome_arquivo):\n",
    "    \"\"\"\n",
    "    Função de tratamento de tipos para os CSVs da FOPAG.\n",
    "    *** VERSÃO CORRIGIDA PARA DATA E FLOAT ***\n",
    "    \"\"\"\n",
    "    print(f\"Iniciando tratamento de tipos para {nome_arquivo}...\")\n",
    "\n",
    "    # --- [CORREÇÃO DATAS] ---\n",
    "    colunas_data = ['competencia', 'data_admissao', 'data_demissao']\n",
    "    for col in colunas_data:\n",
    "        if col in df.columns:\n",
    "            print(f\"Tratando tipo de data: {col}\")\n",
    "            df[col] = clean_text(df[col])\n",
    "            # O CSV já está em formato ISO (YYYY-MM-DD), o pandas lê automaticamente\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce') \n",
    "            df[col] = df[col].dt.date # Converte para objeto date (YYYY-MM-DD)\n",
    "\n",
    "    # --- [CORREÇÃO NUMÉRICOS PARA FLOAT] ---\n",
    "    colunas_monetarias = [\n",
    "        'salario_contratual', 'total_proventos', 'total_descontos',\n",
    "        'valor_liquido', 'base_inss', 'base_fgts', 'valor_fgts',\n",
    "        'base_irrf', 'valor_rubrica'\n",
    "    ]\n",
    "    for col in colunas_monetarias:\n",
    "        if col in df.columns:\n",
    "            print(f\"Tratando tipo: {col} (String -> Float)\")\n",
    "            df[col] = clean_text(df[col]) \n",
    "            # Usa a nova função para_float\n",
    "            df[col] = df[col].apply(para_float)\n",
    "            # Garante que a coluna inteira seja do tipo float no Pandas\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce') \n",
    "\n",
    "    # --- [CPF E TEXTO] ---\n",
    "    if 'cpf' in df.columns:\n",
    "        print(\"Tratando tipo: cpf (String -> String Limpa)\")\n",
    "        df['cpf'] = clean_text(df['cpf'])\n",
    "        df['cpf'] = df['cpf'].str.replace(r'[^\\d]', '', regex=True)\n",
    "\n",
    "    # Adiciona os novos campos de FATO à limpeza de texto\n",
    "    colunas_texto = [\n",
    "        'departamento', 'vinculo', 'nome_funcionario',\n",
    "        'motivo_demissao', 'cargo', 'codigo_rubrica',\n",
    "        'nome_rubrica', 'tipo_rubrica',\n",
    "        \n",
    "        # NOVOS CAMPOS PARA A FATO\n",
    "        'situacao', \n",
    "        'tipo_calculo' \n",
    "    ]\n",
    "    \n",
    "    for col in colunas_texto:\n",
    "        if col in df.columns:\n",
    "            df[col] = clean_text(df[col])\n",
    "\n",
    "    print(\"Tratamento de tipos finalizado.\")\n",
    "    return df\n",
    "# --- [FIM DAS FUNÇÕES HELPER DE CSV CORRIGIDAS] ---\n",
    "\n",
    "\n",
    "def pipeline_fato_folha_consolidada():\n",
    "    print(\"\\n--- Iniciando Pipeline: fato_folha_consolidada ---\")\n",
    "\n",
    "    CSV_FILE = 'BASE_FOPAG_CONSOLIDADA_TOTAIS.csv'\n",
    "    NOME_TABELA_STAGING = 'staging_folha_consolidada'\n",
    "    NOME_TABELA_FINAL = 'fato_folha_consolidada'\n",
    "    NOME_TABELA_BASE_COLAB = 'dim_colaboradores_base' \n",
    "\n",
    "    try:\n",
    "        # 1. Extract\n",
    "        try:\n",
    "            df_csv = pd.read_csv(CSV_FILE, sep=';', dtype=str) # Lê tudo como string\n",
    "        except FileNotFoundError:\n",
    "             print(f\"ERRO: Arquivo '{CSV_FILE}' não encontrado.\")\n",
    "             print(\"Por favor, execute o Notebook 1 (Automação_FOPAG.ipynb) para gerar os CSVs primeiro.\")\n",
    "             return False\n",
    "        except Exception as read_err:\n",
    "            print(f\"Erro ao ler CSV {CSV_FILE}: {read_err}.\")\n",
    "            return False\n",
    "\n",
    "        # 2. Transformação (T)\n",
    "        df_tratado = tratar_tipos_dataframe_csv(df_csv.copy(), CSV_FILE)\n",
    "\n",
    "        # --- [INÍCIO DA ATUALIZAÇÃO] ---\n",
    "        # Popula a dim_colaboradores_base com os dados MESTRE do CSV\n",
    "        print(f\"Extraindo dados mestre do '{CSV_FILE}' para popular a dim_colaboradores_base...\")\n",
    "        if 'cpf' in df_tratado.columns:\n",
    "            # Ordena para pegar os dados mais recentes de cada CPF (baseado na competência)\n",
    "            df_recentes = df_tratado.sort_values(by='competencia', ascending=False).drop_duplicates(subset=['cpf'])\n",
    "            \n",
    "            # Seleciona as colunas mestre\n",
    "            colunas_mestre = [\n",
    "                'cpf', \n",
    "                'nome_funcionario', \n",
    "                'data_admissao', \n",
    "                'data_demissao', \n",
    "                'situacao', \n",
    "                'departamento', # 'departamento' do CSV\n",
    "                'cargo'         # 'cargo' do CSV\n",
    "            ]\n",
    "            \n",
    "            # Garante que todas as colunas existem no DF\n",
    "            colunas_presentes = [col for col in colunas_mestre if col in df_recentes.columns]\n",
    "            df_colabs_unicos = df_recentes[colunas_presentes].copy()\n",
    "            \n",
    "            # Renomeia para o padrão da função helper\n",
    "            df_colabs_unicos = df_colabs_unicos.rename(columns={\n",
    "                'nome_funcionario': 'nome_colaborador',\n",
    "                'data_admissao': 'data_admissao_csv',\n",
    "                'data_demissao': 'data_demissao_csv',\n",
    "                'situacao': 'situacao_csv',\n",
    "                'departamento': 'departamento_csv',\n",
    "                'cargo': 'cargo_csv'\n",
    "            })\n",
    "            \n",
    "            # Garante que a função helper receba colunas mesmo se não existirem no CSV\n",
    "            colunas_helper_esperadas = [\n",
    "                'nome_colaborador', 'cpf', 'data_admissao_csv', 'data_demissao_csv',\n",
    "                'situacao_csv', 'departamento_csv', 'cargo_csv'\n",
    "            ]\n",
    "            for col in colunas_helper_esperadas:\n",
    "                if col not in df_colabs_unicos.columns:\n",
    "                    df_colabs_unicos[col] = None\n",
    "\n",
    "            # --- [INÍCIO DA CORREÇÃO 3/3 - Parte 3] ---\n",
    "            # Captura o retorno da função helper\n",
    "            sucesso_upsert = atualizar_dim_colaboradores_base(engine, df_colabs_unicos, DB_SCHEMA)\n",
    "            \n",
    "            if not sucesso_upsert:\n",
    "                print(\"Falha ao atualizar a dim_colaboradores_base. Abortando pipeline da Fato Consolidada.\")\n",
    "                return False # Para a execução\n",
    "            # --- [FIM DA CORREÇÃO 3/3 - Parte 3] ---\n",
    "\n",
    "        else:\n",
    "            print(\"AVISO: Coluna 'cpf' não encontrada no CSV. Não foi possível popular a dim_colaboradores_base.\")\n",
    "        # --- [FIM DA ATUALIZAÇÃO] ---\n",
    "\n",
    "\n",
    "        # 3. Carga (L)\n",
    "        \n",
    "        # Agora o df_tratado tem colunas float, o to_sql vai criar a staging table\n",
    "        # com os tipos numéricos corretos, sem precisar do dtype_map.\n",
    "        print(f\"Carregando CSV para {NOME_TABELA_STAGING}...\")\n",
    "        df_tratado.to_sql(\n",
    "            NOME_TABELA_STAGING, \n",
    "            engine, \n",
    "            if_exists='replace', \n",
    "            index=False, \n",
    "            schema=DB_SCHEMA\n",
    "        )\n",
    "        print(f\"CSV carregado para {NOME_TABELA_STAGING}.\")\n",
    "        \n",
    "        \n",
    "        competencias_no_df = df_tratado['competencia'].dropna().unique()\n",
    "        if len(competencias_no_df) == 0:\n",
    "            print(\"ERRO CRÍTICO: Nenhuma competência válida encontrada no CSV após o tratamento.\")\n",
    "            return False \n",
    "        \n",
    "        print(f\"Competências a serem carregadas na Fato: {len(competencias_no_df)} meses/períodos.\")\n",
    "\n",
    "        # --- (SQL para popular a Fato - IDÊNTICO) ---\n",
    "        sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS \"{DB_SCHEMA}\".{NOME_TABELA_FINAL} (\n",
    "            fato_folha_id SERIAL PRIMARY KEY,\n",
    "            colaborador_sk INTEGER, \n",
    "            competencia DATE,\n",
    "            nome_funcionario_csv VARCHAR(255), \n",
    "            centro_de_custo VARCHAR(255), \n",
    "            cargo_nome_csv VARCHAR(255),  \n",
    "            cpf_csv VARCHAR(11),\n",
    "            \n",
    "            -- Novos campos de FATO\n",
    "            situacao_csv VARCHAR(100),\n",
    "            tipo_calculo_csv VARCHAR(100),\n",
    "            \n",
    "            -- Métricas\n",
    "            salario_contratual NUMERIC(12, 2),\n",
    "            total_proventos NUMERIC(12, 2),\n",
    "            total_descontos NUMERIC(12, 2),\n",
    "            valor_liquido NUMERIC(12, 2),\n",
    "            base_inss NUMERIC(12, 2),\n",
    "            base_fgts NUMERIC(12, 2),\n",
    "            valor_fgts NUMERIC(12, 2),\n",
    "            base_irrf NUMERIC(12, 2),\n",
    "            FOREIGN KEY (colaborador_sk) REFERENCES \"{DB_SCHEMA}\".{NOME_TABELA_BASE_COLAB}(colaborador_sk)\n",
    "        );\n",
    "\n",
    "        DELETE FROM \"{DB_SCHEMA}\".{NOME_TABELA_FINAL}\n",
    "        WHERE competencia IN :competencias_list;\n",
    "\n",
    "        INSERT INTO \"{DB_SCHEMA}\".{NOME_TABELA_FINAL} (\n",
    "            colaborador_sk, \n",
    "            competencia,\n",
    "            nome_funcionario_csv, centro_de_custo, cargo_nome_csv, cpf_csv,\n",
    "            situacao_csv, tipo_calculo_csv, -- NOVOS\n",
    "            salario_contratual, total_proventos, total_descontos, valor_liquido,\n",
    "            base_inss, base_fgts, valor_fgts, base_irrf\n",
    "        )\n",
    "        SELECT\n",
    "            COALESCE(base.colaborador_sk, 0) AS colaborador_sk,\n",
    "            stg.competencia,\n",
    "            stg.nome_funcionario AS nome_funcionario_csv, \n",
    "            stg.departamento AS centro_de_custo, \n",
    "            stg.cargo AS cargo_nome_csv,        \n",
    "            stg.cpf AS cpf_csv,               \n",
    "\n",
    "            stg.situacao AS situacao_csv,       -- NOVO\n",
    "            stg.tipo_calculo AS tipo_calculo_csv, -- NOVO\n",
    "\n",
    "            stg.salario_contratual, stg.total_proventos, stg.total_descontos, stg.valor_liquido,\n",
    "            stg.base_inss, stg.base_fgts, stg.valor_fgts, stg.base_irrf\n",
    "        FROM\n",
    "            \"{DB_SCHEMA}\".{NOME_TABELA_STAGING} AS stg\n",
    "        LEFT JOIN\n",
    "            \"{DB_SCHEMA}\".{NOME_TABELA_BASE_COLAB} AS base ON stg.cpf = base.cpf\n",
    "        ;\n",
    "        \"\"\"\n",
    "        \n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(sql), {\"competencias_list\": tuple(competencias_no_df)})\n",
    "            \n",
    "        print(f\"Carga na {NOME_TABELA_FINAL} concluída com sucesso!\")\n",
    "        return True\n",
    "\n",
    "    except sqlalchemy_exc.SQLAlchemyError as e: \n",
    "        print(f\"Falha no pipeline {NOME_TABELA_FINAL} (SQLAlchemyError): {e}\")\n",
    "        if hasattr(e, 'orig') and e.orig:\n",
    "             print(f\"  Erro original (psycopg2): {e.orig}\")\n",
    "        return False\n",
    "    except pd.errors.ParserError as e: \n",
    "       print(f\"Falha ao ler o CSV {CSV_FILE}: {e}\")\n",
    "       return False\n",
    "    except Exception as e: \n",
    "        print(f\"Falha no pipeline {NOME_TABELA_FINAL} (Erro genérico): {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def pipeline_fato_folha_detalhada():\n",
    "    print(\"\\n--- Iniciando Pipeline: fato_folha_detalhada ---\")\n",
    "\n",
    "    CSV_FILE = 'BASE_FOPAG_DETALHADA_RUBRICAS.csv'\n",
    "    NOME_TABELA_STAGING = 'staging_folha_detalhada'\n",
    "    NOME_TABELA_FINAL = 'fato_folha_detalhada'\n",
    "    NOME_TABELA_BASE_COLAB = 'dim_colaboradores_base' \n",
    "\n",
    "    try:\n",
    "        # 1. Extract\n",
    "        try:\n",
    "            df_csv = pd.read_csv(CSV_FILE, sep=';', dtype=str) # Lê tudo como string\n",
    "        except FileNotFoundError:\n",
    "             print(f\"ERRO: Arquivo '{CSV_FILE}' não encontrado.\")\n",
    "             print(\"Por favor, execute o Notebook 1 (Automação_FOPAG.ipynb) para gerar os CSVs primeiro.\")\n",
    "             return False\n",
    "        except Exception as read_err:\n",
    "            print(f\"Erro ao ler CSV {CSV_FILE}: {read_err}.\")\n",
    "            return False\n",
    "\n",
    "        # 2. Transformação (T)\n",
    "        df_tratado = tratar_tipos_dataframe_csv(df_csv.copy(), CSV_FILE)\n",
    "        \n",
    "        # --- [INÍCIO DA CORREÇÃO 2/3] ---\n",
    "        # Garante que as colunas de FATO esperadas existam no DataFrame\n",
    "        # antes de carregar para a staging. Isso corrige o Erro 2.\n",
    "        colunas_fato_esperadas = ['situacao', 'tipo_calculo']\n",
    "        for col in colunas_fato_esperadas:\n",
    "            if col not in df_tratado.columns:\n",
    "                print(f\"Aviso: Coluna '{col}' não encontrada no CSV {CSV_FILE}. Será preenchida com Nulo.\")\n",
    "                df_tratado[col] = None\n",
    "        # --- [FIM DA CORREÇÃO 2/3] ---\n",
    "        \n",
    "        \n",
    "        # 3. Carga (L)\n",
    "        \n",
    "        # Converte para float para o to_sql criar a staging table com tipo numérico\n",
    "        print(f\"Carregando CSV para {NOME_TABELA_STAGING}...\")\n",
    "        df_tratado.to_sql(\n",
    "            NOME_TABELA_STAGING, \n",
    "            engine, \n",
    "            if_exists='replace', \n",
    "            index=False, \n",
    "            schema=DB_SCHEMA\n",
    "        )\n",
    "        print(f\"CSV carregado para {NOME_TABELA_STAGING}.\")\n",
    "        \n",
    "        \n",
    "        competencias_no_df = df_tratado['competencia'].dropna().unique()\n",
    "        if len(competencias_no_df) == 0:\n",
    "            print(\"ERRO CRÍTICO: Nenhuma competência válida encontrada no CSV após o tratamento.\")\n",
    "            return False \n",
    "        \n",
    "        print(f\"Competências a serem carregadas na Fato: {len(competencias_no_df)} meses/períodos.\")\n",
    "\n",
    "        # --- (SQL para popular a Fato - IDÊNTICO) ---\n",
    "        sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS \"{DB_SCHEMA}\".{NOME_TABELA_FINAL} (\n",
    "            fato_rubrica_id SERIAL PRIMARY KEY,\n",
    "            colaborador_sk INTEGER, \n",
    "            competencia DATE,\n",
    "            nome_funcionario_csv VARCHAR(255), \n",
    "            centro_de_custo VARCHAR(255), \n",
    "            cpf_csv VARCHAR(11),\n",
    "            \n",
    "            -- Novos campos de FATO\n",
    "            situacao_csv VARCHAR(100),\n",
    "            tipo_calculo_csv VARCHAR(100),\n",
    "            \n",
    "            -- Detalhes da Rubrica\n",
    "            codigo_rubrica VARCHAR(100),\n",
    "            nome_rubrica VARCHAR(255),\n",
    "            tipo_rubrica VARCHAR(100),\n",
    "            valor_rubrica NUMERIC(12, 2),\n",
    "            FOREIGN KEY (colaborador_sk) REFERENCES \"{DB_SCHEMA}\".{NOME_TABELA_BASE_COLAB}(colaborador_sk)\n",
    "        );\n",
    "\n",
    "        DELETE FROM \"{DB_SCHEMA}\".{NOME_TABELA_FINAL}\n",
    "        WHERE competencia IN :competencias_list;\n",
    "\n",
    "        INSERT INTO \"{DB_SCHEMA}\".{NOME_TABELA_FINAL} (\n",
    "            colaborador_sk, \n",
    "            competencia,\n",
    "            nome_funcionario_csv, centro_de_custo, cpf_csv,\n",
    "            situacao_csv, tipo_calculo_csv, -- NOVOS\n",
    "            codigo_rubrica, nome_rubrica, tipo_rubrica, valor_rubrica\n",
    "        )\n",
    "        SELECT\n",
    "            COALESCE(base.colaborador_sk, 0) AS colaborador_sk,\n",
    "            stg.competencia,\n",
    "            stg.nome_funcionario AS nome_funcionario_csv, \n",
    "            stg.departamento AS centro_de_custo, \n",
    "            stg.cpf AS cpf_csv,               \n",
    "            \n",
    "            stg.situacao AS situacao_csv,       -- NOVO\n",
    "            stg.tipo_calculo AS tipo_calculo_csv, -- NOVO\n",
    "            \n",
    "            stg.codigo_rubrica, stg.nome_rubrica, stg.tipo_rubrica, stg.valor_rubrica\n",
    "        FROM\n",
    "            \"{DB_SCHEMA}\".{NOME_TABELA_STAGING} AS stg\n",
    "        LEFT JOIN\n",
    "            \"{DB_SCHEMA}\".{NOME_TABELA_BASE_COLAB} AS base ON stg.cpf = base.cpf\n",
    "        ;\n",
    "        \"\"\"\n",
    "\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(sql), {\"competencias_list\": tuple(competencias_no_df)})\n",
    "            \n",
    "        print(f\"Carga na {NOME_TABELA_FINAL} concluída com sucesso!\")\n",
    "        return True\n",
    "\n",
    "    except sqlalchemy_exc.SQLAlchemyError as e: \n",
    "        print(f\"Falha no pipeline {NOME_TABELA_FINAL} (SQLAlchemyError): {e}\")\n",
    "        if hasattr(e, 'orig') and e.orig:\n",
    "             print(f\"  Erro original (psycopg2): {e.orig}\")\n",
    "        return False\n",
    "    except pd.errors.ParserError as e: \n",
    "       print(f\"Falha ao ler o CSV {CSV_FILE}: {e}\")\n",
    "       return False\n",
    "    except Exception as e: \n",
    "        print(f\"Falha no pipeline {NOME_TABELA_FINAL} (Erro genérico): {e}\")\n",
    "        return False\n",
    "    \n",
    "\n",
    "def processar_status_transferidos():\n",
    "    \"\"\"\n",
    "    Identifica colaboradores que 'sumiram' da carga (API e CSV) sem data de demissão\n",
    "    e atualiza o status para 'Transferido'.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Iniciando Pós-Processamento: Identificação de Transferidos ---\")\n",
    "    \n",
    "    # Nomes das tabelas\n",
    "    TB_BASE = f'\"{DB_SCHEMA}\".dim_colaboradores_base'\n",
    "    TB_RICA = f'\"{DB_SCHEMA}\".dim_colaboradores'\n",
    "    TB_STG_API = f'\"{DB_SCHEMA}\".staging_colaboradores'\n",
    "    TB_STG_CSV = f'\"{DB_SCHEMA}\".staging_folha_consolidada'\n",
    "\n",
    "    # SQL Lógico:\n",
    "    # 1. Pegar quem está na BASE e NÃO tem data de demissão.\n",
    "    # 2. Verificar se esse CPF NÃO está na Staging da API (carga de hoje).\n",
    "    # 3. Verificar se esse CPF NÃO está na Staging do CSV (carga do mês).\n",
    "    # 4. Se atender a tudo, marca como Transferido.\n",
    "\n",
    "    sql_update = text(f\"\"\"\n",
    "        UPDATE {TB_BASE}\n",
    "        SET \n",
    "            situacao_csv = 'Transferido'\n",
    "        WHERE cpf IN (\n",
    "            -- Seleciona CPFs candidatos a Transferência\n",
    "            SELECT base.cpf\n",
    "            FROM {TB_BASE} base\n",
    "            -- Que NÃO estão na API hoje\n",
    "            LEFT JOIN {TB_STG_API} api ON base.cpf = api.cpf\n",
    "            -- Que NÃO estão no CSV hoje\n",
    "            LEFT JOIN {TB_STG_CSV} csv ON base.cpf = csv.cpf\n",
    "            WHERE \n",
    "                api.cpf IS NULL              -- Sumiu da API\n",
    "                AND csv.cpf IS NULL          -- Sumiu do CSV\n",
    "                AND base.data_demissao_csv IS NULL -- Não foi demitido oficialmente\n",
    "                AND base.situacao_csv != 'Transferido' -- Já não é transferido\n",
    "                AND base.situacao_csv != 'Desligado'   -- Já não é desligado\n",
    "        );\n",
    "    \"\"\")\n",
    "    \n",
    "    # Atualiza também a Dimensão Rica para refletir a mudança\n",
    "    sql_sync_rica = text(f\"\"\"\n",
    "        UPDATE {TB_RICA}\n",
    "        SET \n",
    "            ativo = False, -- Transferido não conta como ativo na unidade atual\n",
    "            data_ultima_atualizacao = current_timestamp\n",
    "        FROM {TB_BASE} base\n",
    "        WHERE {TB_RICA}.colaborador_sk = base.colaborador_sk\n",
    "        AND base.situacao_csv = 'Transferido'\n",
    "        AND {TB_RICA}.ativo = True; -- Só atualiza se ainda constava como ativo\n",
    "    \"\"\")\n",
    "\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            # Executa a marcação na Base\n",
    "            result = conn.execute(sql_update)\n",
    "            afetados = result.rowcount\n",
    "            print(f\"Detectados e marcados como 'Transferido' na Base: {afetados} colaboradores.\")\n",
    "            \n",
    "            # Sincroniza a Rica\n",
    "            if afetados > 0:\n",
    "                result_rica = conn.execute(sql_sync_rica)\n",
    "                print(f\"Status 'Ativo' atualizado para False na Dimensão Rica: {result_rica.rowcount} registros.\")\n",
    "            \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar transferidos: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# --- PONTO DE EXECUÇÃO PRINCIPAL --\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Ordem de execution é crucial\n",
    "\n",
    "    # 1. Dimensões independentes\n",
    "    sucesso_colab = pipeline_dim_colaboradores() # Popula a Base com dados da API\n",
    "    sucesso_calendario = pipeline_dim_calendario() \n",
    "\n",
    "    # 2. Fatos (dependentes)\n",
    "    # Agora, o pipeline da FATO irá *primeiro* popular a Base com dados do CSV\n",
    "    # e *depois* carregar a Fato.\n",
    "    \n",
    "    if sucesso_colab and sucesso_calendario:\n",
    "        # A Consolidada AGORA atualiza a dim_colaboradores_base\n",
    "        sucesso_fato_cons = pipeline_fato_folha_consolidada()\n",
    "        \n",
    "        # A Detalhada apenas lê da dim_colaboradores_base\n",
    "        # Ela só executa se a consolidada (que atualiza a base) funcionar\n",
    "        if sucesso_fato_cons:\n",
    "            sucesso_fato_det = pipeline_fato_folha_detalhada()\n",
    "            # --- NOVO PASSO: RODAR APÓS AS CARGAS ---\n",
    "            processar_status_transferidos()\n",
    "            # ----------------------------------------\n",
    "        else:\n",
    "            sucesso_fato_det = False # Pula a execução\n",
    "        \n",
    "        if not sucesso_fato_cons or not sucesso_fato_det:\n",
    "             print(\"\\n!!! Atenção: Pelo menos um pipeline de FATO falhou. Verifique os logs acima. !!!\")\n",
    "        else:\n",
    "             print(\"\\n--- Pipeline ETL Concluído com Sucesso! ---\")\n",
    "\n",
    "    else:\n",
    "        if not sucesso_colab:\n",
    "             print(\"\\nFalha ao carregar dim_colaboradores (API). Abortando pipelines de Fatos.\")\n",
    "        if not sucesso_calendario:\n",
    "             print(\"\\nFalha ao carregar dim_calendario. Abortando pipelines de Fatos.\")\n",
    "        sys.exit() # Encerra se as dimensões falharem\n",
    "\n",
    "\n",
    "    # Fecha a conexão com o banco\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680d1e45",
   "metadata": {},
   "source": [
    "# Debug JSON\n",
    "\n",
    "Nessa célula entendo os pontos do JSON (Benefícios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78d09044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando ETL...\n",
      "Conexão com PostgreSQL estabelecida e schema '\"FOPAG\"' garantido.\n",
      "\n",
      "--- Iniciando Pipeline: dim_colaboradores & fatos_beneficios ---\n",
      "Iniciando extração (Passo 1/2): Buscando lista de IDs de colaboradores...\n",
      "Página 1 da lista carregada...\n",
      "Página 2 da lista carregada...\n",
      "Extração da lista concluída. Total de 139 colaboradores encontrados.\n",
      "Passo 1/2 concluído. 139 colaboradores encontrados.\n",
      "Iniciando extração (Passo 2/2): Buscando detalhes completos...\n",
      "   Buscando colaborador 50 de 139...\n",
      "   Buscando colaborador 100 de 139...\n",
      "Passo 2/2 concluído.\n",
      "Processando lista de benefícios...\n",
      "Benefícios extraídos: 470 registros.\n",
      "Transformação de colaboradores concluída.\n",
      "Carregando staging_beneficios_api...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JoãoPedrodosSantosSa\\AppData\\Local\\Temp\\ipykernel_17352\\2666277755.py:394: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  df[col] = pd.to_datetime(df[col], dayfirst=True, errors='coerce').dt.date\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carga na dim_colaboradores_base, dim_colaboradores e fato_beneficios_api concluída com sucesso!\n",
      "\n",
      "--- Iniciando Pipeline: dim_calendario ---\n",
      "Carga na dim_calendario concluída com sucesso!\n",
      "\n",
      "--- Iniciando Pipeline: fato_folha_consolidada ---\n",
      "Iniciando tratamento de tipos para BASE_FOPAG_CONSOLIDADA_TOTAIS.csv...\n",
      "Tratando tipo de data: competencia\n",
      "Tratando tipo de data: data_admissao\n",
      "Tratando tipo de data: data_demissao\n",
      "Tratando tipo: salario_contratual (String -> Float)\n",
      "Tratando tipo: total_proventos (String -> Float)\n",
      "Tratando tipo: total_descontos (String -> Float)\n",
      "Tratando tipo: valor_liquido (String -> Float)\n",
      "Tratando tipo: base_inss (String -> Float)\n",
      "Tratando tipo: base_fgts (String -> Float)\n",
      "Tratando tipo: valor_fgts (String -> Float)\n",
      "Tratando tipo: base_irrf (String -> Float)\n",
      "Tratando tipo: cpf (String -> String Limpa)\n",
      "Tratamento de tipos finalizado.\n",
      "Extraindo dados mestre do 'BASE_FOPAG_CONSOLIDADA_TOTAIS.csv' para popular a dim_colaboradores_base...\n",
      "\n",
      "--- Iniciando UPSERT para 'dim_colaboradores_base' (Tabela Mestre) ---\n",
      "SUCESSO! 'dim_colaboradores_base' (Mestre) foi atualizada com os dados do DataFrame.\n",
      "Carregando CSV para staging_folha_consolidada...\n",
      "CSV carregado para staging_folha_consolidada.\n",
      "Competências a serem carregadas na Fato: 41 meses/períodos.\n",
      "Carga na fato_folha_consolidada concluída com sucesso!\n",
      "\n",
      "--- Iniciando Pipeline: fato_folha_detalhada ---\n",
      "Iniciando tratamento de tipos para BASE_FOPAG_DETALHADA_RUBRICAS.csv...\n",
      "Tratando tipo de data: competencia\n",
      "Tratando tipo: valor_rubrica (String -> Float)\n",
      "Tratando tipo: cpf (String -> String Limpa)\n",
      "Tratamento de tipos finalizado.\n",
      "Aviso: Coluna 'situacao' não encontrada no CSV BASE_FOPAG_DETALHADA_RUBRICAS.csv. Será preenchida com Nulo.\n",
      "Carregando CSV para staging_folha_detalhada...\n",
      "CSV carregado para staging_folha_detalhada.\n",
      "Competências a serem carregadas na Fato: 41 meses/períodos.\n",
      "Carga na fato_folha_detalhada concluída com sucesso!\n",
      "\n",
      "--- Iniciando Pós-Processamento: Identificação de Transferidos ---\n",
      "Detectados e marcados como 'Transferido' na Base: 0 colaboradores.\n",
      "\n",
      "--- Pipeline ETL Concluído com Sucesso! ---\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text, exc as sqlalchemy_exc\n",
    "# --- [INÍCIO DAS IMPORTAÇÕES CORRIGIDAS] ---\n",
    "import sys\n",
    "import numpy as np \n",
    "import json \n",
    "from decimal import Decimal, InvalidOperation\n",
    "from sqlalchemy.types import String, Date, Numeric # Usado apenas para a API, mas mantido\n",
    "# --- [FIM DAS IMPORTAÇÕES CORRIGIDAS] ---\n",
    "\n",
    "\n",
    "# 1. CARREGAR VARIÁVEIS DE AMBIENTE\n",
    "# -----------------------------------\n",
    "print(\"Iniciando ETL...\")\n",
    "load_dotenv()\n",
    "\n",
    "# Carrega o Token da API\n",
    "API_TOKEN = os.getenv('SOLIDES_API_TOKEN')\n",
    "\n",
    "# Carrega os componentes do Banco\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASS = os.getenv('DB_PASS')\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_PORT = os.getenv('DB_PORT')\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "DB_SCHEMA = os.getenv('DB_SCHEMA')\n",
    "\n",
    "# Verifica se tudo foi carregado\n",
    "if not all([API_TOKEN, DB_USER, DB_PASS, DB_HOST, DB_PORT, DB_NAME, DB_SCHEMA]):\n",
    "    print(\"ERRO: Faltando uma ou mais variáveis no arquivo .env\")\n",
    "    print(f\"API_TOKEN Carregado: {'Sim' if API_TOKEN else 'NÃO'}\")\n",
    "    print(f\"DB_SCHEMA Carregado: {DB_SCHEMA}\")\n",
    "    sys.exit() # Encerra o script se faltar configuração\n",
    "\n",
    "# 2. CONFIGURAÇÕES GLOBAIS\n",
    "# -----------------------------------\n",
    "DB_URL = f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "BASE_URL = \"https://app.solides.com/pt-BR/api/v1\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Token token={API_TOKEN}\",\n",
    "    \"Accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "# 3. CRIA A CONEXÃO E GARANTE O SCHEMA (COM ASPAS)\n",
    "# ----------------------------------------------------\n",
    "try:\n",
    "    engine = create_engine(DB_URL)\n",
    "    with engine.begin() as conn:\n",
    "        \n",
    "        # 1. Garante que o schema (\"FOPAG\") existe PRIMEIRO.\n",
    "        conn.execute(text(f'CREATE SCHEMA IF NOT EXISTS \\\"{DB_SCHEMA}\\\"'))\n",
    "        \n",
    "        # 2. Instala a extensão explicitamente DENTRO do seu schema.\n",
    "        conn.execute(text(f'CREATE EXTENSION IF NOT EXISTS unaccent WITH SCHEMA \\\"{DB_SCHEMA}\\\";'))\n",
    "\n",
    "    # Recria a engine, definindo o search_path\n",
    "    engine = create_engine(\n",
    "        DB_URL,\n",
    "        connect_args={'options': f'-csearch_path=\\\"{DB_SCHEMA}\\\"'}\n",
    "    )\n",
    "\n",
    "    print(f\"Conexão com PostgreSQL estabelecida e schema '\\\"{DB_SCHEMA}\\\"' garantido.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao conectar ao PostgreSQL ou criar schema: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "# --- FUNÇÕES HELPER ---\n",
    "\n",
    "def limpar_salario_api(salario_str):\n",
    "    \"\"\"Limpa a string de salário vinda da API (ex: \"R$ 8.200,00\") para float.\"\"\"\n",
    "    if salario_str is None or pd.isna(salario_str):\n",
    "        return np.nan\n",
    "    try:\n",
    "        # Remove 'R$', espaços, e usa '.' como separador de milhar\n",
    "        salario_limpo = str(salario_str).replace('R$', '').replace(' ', '').replace('.', '')\n",
    "        # Troca ',' por '.' para ser decimal\n",
    "        salario_limpo = salario_limpo.replace(',', '.')\n",
    "        return pd.to_numeric(salario_limpo, errors='coerce')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# --- [INÍCIO DA ATUALIZAÇÃO] ---\n",
    "# A 'dim_colaboradores_base' agora é a dimensão MESTRE\n",
    "def atualizar_dim_colaboradores_base(engine, df_colaboradores, schema_name):\n",
    "    \"\"\"\n",
    "    Cria a tabela dim_colaboradores_base (se não existir) e\n",
    "    faz o UPSERT (INSERT ... ON CONFLICT) dos dados de colaboradores.\n",
    "    AGORA, esta tabela contém os dados mestres vindos do CSV.\n",
    "    \"\"\"\n",
    "    NOME_TABELA_BASE = \"dim_colaboradores_base\"\n",
    "    NOME_TABELA_STAGING_TEMP = \"stg_colab_temp_upsert\" \n",
    "\n",
    "    if df_colaboradores is None or df_colaboradores.empty:\n",
    "        print(\"Nenhum dado de colaborador fornecido para o UPSERT.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- Iniciando UPSERT para '{NOME_TABELA_BASE}' (Tabela Mestre) ---\")\n",
    "\n",
    "    # SQL para criar a tabela base (AGORA ENRIQUECIDA)\n",
    "    sql_create_base = text(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS \\\"{schema_name}\\\".\\\"{NOME_TABELA_BASE}\\\" (\n",
    "            colaborador_sk SERIAL PRIMARY KEY,\n",
    "            nome_colaborador VARCHAR(255) NOT NULL,\n",
    "            cpf VARCHAR(20) UNIQUE NOT NULL,\n",
    "            \n",
    "            -- Novos campos mestres (do CSV)\n",
    "            data_admissao_csv DATE,\n",
    "            data_demissao_csv DATE,\n",
    "            situacao_csv VARCHAR(100),\n",
    "            departamento_csv VARCHAR(255),\n",
    "            cargo_csv VARCHAR(255)\n",
    "        );\n",
    "        INSERT INTO \\\"{schema_name}\\\".\\\"{NOME_TABELA_BASE}\\\" (colaborador_sk, nome_colaborador, cpf)\n",
    "        VALUES (0, 'Desconhecido', 'N/A')\n",
    "        ON CONFLICT (colaborador_sk) DO NOTHING;\n",
    "    \"\"\")\n",
    "\n",
    "    # SQL de UPSERT (AGORA ENRIQUECIDO)\n",
    "    sql_upsert = text(f\"\"\"\n",
    "        INSERT INTO \\\"{schema_name}\\\".\\\"{NOME_TABELA_BASE}\\\" (\n",
    "            nome_colaborador, cpf, \n",
    "            data_admissao_csv, data_demissao_csv, situacao_csv, \n",
    "            departamento_csv, cargo_csv\n",
    "        )\n",
    "        SELECT\n",
    "            DISTINCT ON (src.cpf)\n",
    "            src.nome_colaborador,\n",
    "            src.cpf,\n",
    "            src.data_admissao_csv,\n",
    "            src.data_demissao_csv,\n",
    "            src.situacao_csv,\n",
    "            src.departamento_csv,\n",
    "            src.cargo_csv\n",
    "        FROM\n",
    "            \\\"{schema_name}\\\".\\\"{NOME_TABELA_STAGING_TEMP}\\\" AS src\n",
    "        WHERE\n",
    "            src.cpf IS NOT NULL AND src.cpf != 'N/A'\n",
    "        ORDER BY\n",
    "            src.cpf, src.nome_colaborador DESC\n",
    "        ON CONFLICT (cpf) DO UPDATE SET\n",
    "            nome_colaborador = EXCLUDED.nome_colaborador,\n",
    "            data_admissao_csv = EXCLUDED.data_admissao_csv,\n",
    "            data_demissao_csv = EXCLUDED.data_demissao_csv,\n",
    "            situacao_csv = EXCLUDED.situacao_csv,\n",
    "            departamento_csv = EXCLUDED.departamento_csv,\n",
    "            cargo_csv = EXCLUDED.cargo_csv;\n",
    "    \"\"\")\n",
    "    \n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            # 1. Cria a tabela base (se não existir) com a NOVA ESTRUTURA\n",
    "            conn.execute(sql_create_base)\n",
    "\n",
    "            # 2. Carga dos dados do DataFrame para a tabela temporária de staging\n",
    "            # O DataFrame já deve vir com os nomes de colunas corretos \n",
    "            # (ex: 'data_admissao_csv')\n",
    "            df_colaboradores.to_sql(\n",
    "                NOME_TABELA_STAGING_TEMP,\n",
    "                con=conn,\n",
    "                schema=schema_name,\n",
    "                if_exists='replace',\n",
    "                index=False\n",
    "            )\n",
    "\n",
    "            # 3. Executa o UPSERT (agora enriquecido)\n",
    "            conn.execute(sql_upsert)\n",
    "\n",
    "            # 4. (Opcional) Limpa a tabela temporária\n",
    "            conn.execute(text(f\"DROP TABLE \\\"{schema_name}\\\".\\\"{NOME_TABELA_STAGING_TEMP}\\\"\"))\n",
    "\n",
    "        print(f\"SUCESSO! '{NOME_TABELA_BASE}' (Mestre) foi atualizada com os dados do DataFrame.\")\n",
    "        # --- [INÍCIO DA CORREÇÃO 3/3 - Parte 1] ---\n",
    "        return True # <-- Retorna Sucesso\n",
    "        # --- [FIM DA CORREÇÃO 3/3 - Parte 1] ---\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERRO ao fazer UPSERT na '{NOME_TABELA_BASE}': {e}\")\n",
    "        # --- [INÍCIO DA CORREÇÃO 3/3 - Parte 2] ---\n",
    "        return False # <-- Retorna Falha\n",
    "        # --- [FIM DA CORREÇÃO 3/3 - Parte 2] ---\n",
    "# --- [FIM DA ATUALIZAÇÃO] ---\n",
    "\n",
    "\n",
    "# --- FASE 1: PIPELINES DAS DIMENSÕES (API) ---\n",
    "def pipeline_dim_colaboradores():\n",
    "    \"\"\"\n",
    "    PUXA dados de Colaboradores da API (paginado) e carrega na dim_colaboradores.\n",
    "    TAMBÉM extrai a lista detalhada de benefícios para a tabela 'fato_beneficios_api'.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Iniciando Pipeline: dim_colaboradores & fatos_beneficios ---\")\n",
    "\n",
    "    # 1. Extração (E)\n",
    "    all_colaboradores_lista = []\n",
    "    page = 1\n",
    "    page_size = 100\n",
    "    ENDPOINT_LISTA = \"/colaboradores\" \n",
    "    print(\"Iniciando extração (Passo 1/2): Buscando lista de IDs de colaboradores...\")\n",
    "    while True:\n",
    "        params = {'page': page, 'page_size': page_size, 'status': 'todos'} \n",
    "        try:\n",
    "            response = requests.get(f\"{BASE_URL}{ENDPOINT_LISTA}\", headers=HEADERS, params=params)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if not data:\n",
    "                    print(f\"Extração da lista concluída. Total de {len(all_colaboradores_lista)} colaboradores encontrados.\")\n",
    "                    break\n",
    "                all_colaboradores_lista.extend(data) \n",
    "                print(f\"Página {page} da lista carregada...\")\n",
    "                page += 1\n",
    "            else:\n",
    "                print(f\"Erro na API (Página {page}): {response.status_code} {response.text}\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na extração de colaboradores (lista): {e}\")\n",
    "            return False\n",
    "            \n",
    "    if not all_colaboradores_lista:\n",
    "        print(\"Nenhum colaborador encontrado.\")\n",
    "        return True\n",
    "\n",
    "    print(f\"Passo 1/2 concluído. {len(all_colaboradores_lista)} colaboradores encontrados.\")\n",
    "    all_colaboradores_detalhado = []\n",
    "    total_colabs = len(all_colaboradores_lista)\n",
    "    print(f\"Iniciando extração (Passo 2/2): Buscando detalhes completos...\")\n",
    "    \n",
    "    for i, colab_info in enumerate(all_colaboradores_lista):\n",
    "        colab_id = colab_info.get('id')\n",
    "        if not colab_id: continue\n",
    "        \n",
    "        # Log a cada 50 para não poluir\n",
    "        if (i+1) % 50 == 0:\n",
    "            print(f\"   Buscando colaborador {i+1} de {total_colabs}...\")\n",
    "\n",
    "        ENDPOINT_DETALHE = f\"/colaboradores/{colab_id}\"\n",
    "        try:\n",
    "            response_detalhe = requests.get(f\"{BASE_URL}{ENDPOINT_DETALHE}\", headers=HEADERS)\n",
    "            if response_detalhe.status_code == 200:\n",
    "                data_detalhe = response_detalhe.json()\n",
    "                all_colaboradores_detalhado.append(data_detalhe)\n",
    "            else:\n",
    "                print(f\"    ERRO ID {colab_id}: {response_detalhe.status_code}. Usando dados básicos.\")\n",
    "                all_colaboradores_detalhado.append(colab_info) \n",
    "        except Exception as e:\n",
    "            print(f\"    EXCEÇÃO ID {colab_id}: {e}. Usando dados básicos.\")\n",
    "            all_colaboradores_detalhado.append(colab_info) \n",
    "    print(\"Passo 2/2 concluído.\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # 2. Transformação (T)\n",
    "    # =========================================================================\n",
    "    \n",
    "    # --- 2.1 Processamento dos Benefícios ---\n",
    "    print(\"Processando lista de benefícios...\")\n",
    "    lista_beneficios = []\n",
    "\n",
    "    for colab in all_colaboradores_detalhado:\n",
    "        colab_id = colab.get('id')\n",
    "        # A chave 'benefits' contém a lista\n",
    "        benefits_data = colab.get('benefits', [])\n",
    "        \n",
    "        if isinstance(benefits_data, list):\n",
    "            for ben in benefits_data:\n",
    "                lista_beneficios.append({\n",
    "                    'colaborador_id_solides': colab_id,\n",
    "                    'nome_beneficio': ben.get('benefitName'),\n",
    "                    'tipo_beneficio': ben.get('typeBenefit'), \n",
    "                    'valor_bruto': ben.get('value'),\n",
    "                    'valor_desconto_bruto': ben.get('valueDiscount'),\n",
    "                    'periodicidade': ben.get('dates'),\n",
    "                    'opcao_desconto': ben.get('discountOption'),\n",
    "                    'aplicado_como': ben.get('benefitAppliedAs')\n",
    "                })\n",
    "\n",
    "    df_beneficios = pd.DataFrame(lista_beneficios)\n",
    "    \n",
    "    if not df_beneficios.empty:\n",
    "        df_beneficios['valor_beneficio'] = df_beneficios['valor_bruto'].apply(limpar_salario_api)\n",
    "        df_beneficios['valor_desconto'] = df_beneficios['valor_desconto_bruto'].apply(limpar_salario_api)\n",
    "        df_beneficios.drop(columns=['valor_bruto', 'valor_desconto_bruto'], inplace=True)\n",
    "    else:\n",
    "        df_beneficios = pd.DataFrame(columns=[\n",
    "            'colaborador_id_solides', 'nome_beneficio', 'tipo_beneficio', \n",
    "            'valor_beneficio', 'valor_desconto', 'periodicidade', \n",
    "            'opcao_desconto', 'aplicado_como'\n",
    "        ])\n",
    "\n",
    "    print(f\"Benefícios extraídos: {len(df_beneficios)} registros.\")\n",
    "\n",
    "    # --- 2.2 Transformação dos Colaboradores ---\n",
    "    df = pd.json_normalize(all_colaboradores_detalhado)\n",
    "\n",
    "    df['dept_name_temp'] = None\n",
    "    if 'departament.name' in df.columns: df['dept_name_temp'] = df['departament.name']\n",
    "    elif 'department.name' in df.columns: df['dept_name_temp'] = df['department.name']\n",
    "    \n",
    "    df['cargo_name_temp'] = None\n",
    "    if 'position.name' in df.columns: df['cargo_name_temp'] = df['position.name']\n",
    "    elif 'cargo.name' in df.columns: df['cargo_name_temp'] = df['cargo.name']\n",
    "    \n",
    "    df['education_level_temp'] = None\n",
    "    cols_educ = ['education', 'educationLevel', 'scholarship', 'schooling', 'escolaridade']\n",
    "    for c in cols_educ:\n",
    "        if c in df.columns and df['education_level_temp'].isnull().all(): df['education_level_temp'] = df[c]\n",
    "\n",
    "    df['cpf_temp'] = None \n",
    "    cols_cpf = ['documents.idNumber', 'documents.cpf', 'idNumber', 'cpf', 'document']\n",
    "    for c in cols_cpf:\n",
    "        if c in df.columns and df['cpf_temp'].isnull().all(): df['cpf_temp'] = df[c]\n",
    "    \n",
    "    if 'cpf_temp' in df.columns:\n",
    "         df['cpf_temp'] = df['cpf_temp'].astype(str).str.replace(r'\\D', '', regex=True)\n",
    "         df['cpf_temp'] = df['cpf_temp'].replace(r'^\\s*$', np.nan, regex=True).replace('None', np.nan).replace('nan', np.nan)\n",
    "    else:\n",
    "         df['cpf_temp'] = None\n",
    "\n",
    "    if 'salary' in df.columns: df['salario_api_temp'] = df['salary'].apply(limpar_salario_api)\n",
    "    else: df['salario_api_temp'] = np.nan\n",
    "        \n",
    "    df = df.rename(columns={\n",
    "        'id': 'colaborador_id_solides',\n",
    "        'name': 'nome_completo',\n",
    "        'cpf_temp': 'cpf', \n",
    "        'birthDate': 'data_nascimento',\n",
    "        'gender': 'genero',\n",
    "        'dateAdmission': 'data_admissao',\n",
    "        'dateDismissal': 'data_demissao',\n",
    "        'active': 'ativo',\n",
    "        'dept_name_temp': 'departamento_nome_api', \n",
    "        'cargo_name_temp': 'cargo_nome_api',           \n",
    "        'email': 'email',\n",
    "        'contact.phone': 'telefone_pessoal', \n",
    "        'contact.cellPhone': 'celular', \n",
    "        'nationality': 'nacionalidade',\n",
    "        'education_level_temp': 'nivel_educacional', \n",
    "        'motherName': 'nome_mae',\n",
    "        'fatherName': 'nome_pai',\n",
    "        'address.streetName': 'logradouro', \n",
    "        'address.number': 'numero_endereco',\n",
    "        'address.additionalInformation': 'complemento_endereco', \n",
    "        'address.neighborhood': 'bairro',\n",
    "        'address.city.name': 'cidade', \n",
    "        'address.state.initials': 'estado', \n",
    "        'address.zipCode': 'cep',\n",
    "        'registration': 'matricula',\n",
    "        'maritalStatus': 'estado_civil',\n",
    "        'salario_api_temp': 'salario_api',\n",
    "        'workShift': 'turno_trabalho',\n",
    "        'typeContract': 'tipo_contrato',\n",
    "        'course': 'curso_formacao',\n",
    "        'hierarchicalLevel': 'nivel_hierarquico',\n",
    "        'senior.name': 'nome_lider_imediato',\n",
    "        'ethnicity': 'etnia',\n",
    "        'unity.name': 'unidade_nome',\n",
    "        'salutation': 'saudacao',\n",
    "        'typeOfSpecialNeed': 'tipo_necessidade_especial',\n",
    "        'birthplace': 'local_nascimento',\n",
    "        'disabledPerson': 'pcd',\n",
    "        'reasonDismissal': 'motivo_demissao_api',\n",
    "        'dateContract': 'data_contrato',\n",
    "        'durationContract': 'duracao_contrato',\n",
    "        'contractExpirationDate': 'data_expiracao_contrato',\n",
    "        'experiencePeriod': 'periodo_experiencia_dias',\n",
    "        'formDismissal': 'forma_demissao',\n",
    "        'decisionDismissal': 'decisao_demissao',\n",
    "        'terminationAmount': 'valor_rescisao',\n",
    "        'totalBenefits': 'total_beneficios_api',\n",
    "        'updated_at': 'data_ultima_atualizacao_api',\n",
    "        'contact.emergencyPhoneNumber': 'telefone_emergencia',\n",
    "        'contact.personalEmail': 'email_pessoal',\n",
    "        'contact.corporateEmail': 'email_corporativo_sec',\n",
    "        'position.id': 'cargo_id_solides',\n",
    "        'departament.id': 'departamento_id_solides',\n",
    "        'documents.rg': 'rg',\n",
    "        'documents.dispatchDate': 'data_emissao_rg',\n",
    "        'documents.issuingBody': 'orgao_emissor_rg',\n",
    "        'documents.voterRegistration': 'titulo_eleitor',\n",
    "        'documents.electoralZone': 'zona_eleitoral',\n",
    "        'documents.electoralSection': 'secao_eleitoral',\n",
    "        'documents.ctpsNum': 'ctps_numero',\n",
    "        'documents.ctpsSerie': 'ctps_serie',\n",
    "        'documents.pis': 'pis'\n",
    "    })\n",
    "    \n",
    "    # Tratamento de Tipos (CORRIGIDO dayfirst=True)\n",
    "    date_cols = ['data_nascimento', 'data_admissao', 'data_demissao', 'data_contrato', 'data_expiracao_contrato', 'data_emissao_rg', 'data_ultima_atualizacao_api']\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            # dayfirst=True corrige o aviso de parsing para datas formato BR (DD/MM/AAAA)\n",
    "            df[col] = pd.to_datetime(df[col], dayfirst=True, errors='coerce').dt.date \n",
    "            \n",
    "    numeric_cols = ['valor_rescisao', 'total_beneficios_api', 'periodo_experiencia_dias', 'cargo_id_solides', 'departamento_id_solides']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    if 'pcd' in df.columns: df['pcd'] = df['pcd'].astype('boolean')\n",
    "            \n",
    "    colunas_staging = [\n",
    "        'colaborador_id_solides', 'cpf', 'nome_completo', 'data_nascimento', 'genero',\n",
    "        'data_admissao', 'data_demissao', 'ativo', 'departamento_nome_api', 'cargo_nome_api',\n",
    "        'email', 'telefone_pessoal', 'celular', 'nacionalidade', 'nivel_educacional',\n",
    "        'nome_mae', 'nome_pai', 'logradouro', 'numero_endereco', 'complemento_endereco', 'bairro', 'cidade', 'estado', 'cep',\n",
    "        'matricula', 'estado_civil', 'salario_api', 'turno_trabalho', 'tipo_contrato',\n",
    "        'curso_formacao', 'nivel_hierarquico', 'nome_lider_imediato', 'etnia', 'unidade_nome',\n",
    "        'saudacao', 'tipo_necessidade_especial', 'local_nascimento', 'pcd', \n",
    "        'motivo_demissao_api', 'data_contrato', 'duracao_contrato', \n",
    "        'data_expiracao_contrato', 'periodo_experiencia_dias', 'forma_demissao',\n",
    "        'decisao_demissao', 'valor_rescisao', 'total_beneficios_api', \n",
    "        'data_ultima_atualizacao_api', 'telefone_emergencia', 'email_pessoal',\n",
    "        'email_corporativo_sec', 'cargo_id_solides', 'departamento_id_solides',\n",
    "        'rg', 'data_emissao_rg', 'orgao_emissor_rg', 'titulo_eleitor', \n",
    "        'zona_eleitoral', 'secao_eleitoral', 'ctps_numero', 'ctps_serie', 'pis'\n",
    "    ]\n",
    "    \n",
    "    for col in colunas_staging:\n",
    "        if col not in df.columns: df[col] = None \n",
    "    df_staging = df[colunas_staging].copy()\n",
    "    print(\"Transformação de colaboradores concluída.\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # 3. Carga (L)\n",
    "    # =========================================================================\n",
    "    NOME_TABELA_RICA = \"dim_colaboradores\"\n",
    "    NOME_TABELA_BASE = \"dim_colaboradores_base\"\n",
    "    NOME_TABELA_STAGING = \"staging_colaboradores\"\n",
    "    NOME_STAGING_BEN = \"staging_beneficios_api\"\n",
    "    NOME_FATO_BEN = \"fato_beneficios_api\"\n",
    "\n",
    "    try:\n",
    "        df_staging.to_sql(NOME_TABELA_STAGING, engine, if_exists='replace', index=False, schema=DB_SCHEMA)\n",
    "        \n",
    "        print(f\"Carregando {NOME_STAGING_BEN}...\")\n",
    "        df_beneficios.to_sql(NOME_STAGING_BEN, engine, if_exists='replace', index=False, schema=DB_SCHEMA)\n",
    "\n",
    "        # SQL\n",
    "        sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS \"{DB_SCHEMA}\".{NOME_TABELA_BASE} (\n",
    "            colaborador_sk SERIAL PRIMARY KEY,\n",
    "            nome_colaborador VARCHAR(255) NOT NULL,\n",
    "            cpf VARCHAR(20) UNIQUE NOT NULL,\n",
    "            data_admissao_csv DATE,\n",
    "            data_demissao_csv DATE,\n",
    "            situacao_csv VARCHAR(100),\n",
    "            departamento_csv VARCHAR(255),\n",
    "            cargo_csv VARCHAR(255)\n",
    "        );\n",
    "        INSERT INTO \"{DB_SCHEMA}\".{NOME_TABELA_BASE} (colaborador_sk, nome_colaborador, cpf)\n",
    "        VALUES (0, 'Desconhecido', 'N/A')\n",
    "        ON CONFLICT (colaborador_sk) DO NOTHING;\n",
    "\n",
    "        ALTER TABLE \"{DB_SCHEMA}\".{NOME_TABELA_BASE}\n",
    "            ADD COLUMN IF NOT EXISTS data_admissao_csv DATE,\n",
    "            ADD COLUMN IF NOT EXISTS data_demissao_csv DATE,\n",
    "            ADD COLUMN IF NOT EXISTS situacao_csv VARCHAR(100),\n",
    "            ADD COLUMN IF NOT EXISTS departamento_csv VARCHAR(255),\n",
    "            ADD COLUMN IF NOT EXISTS cargo_csv VARCHAR(255);\n",
    "\n",
    "        INSERT INTO \"{DB_SCHEMA}\".{NOME_TABELA_BASE} (nome_colaborador, cpf)\n",
    "        SELECT DISTINCT ON (stg.cpf) stg.nome_completo, stg.cpf\n",
    "        FROM \"{DB_SCHEMA}\".{NOME_TABELA_STAGING} AS stg\n",
    "        WHERE stg.cpf IS NOT NULL AND stg.cpf != 'N/A'\n",
    "        ORDER BY stg.cpf, stg.colaborador_id_solides DESC \n",
    "        ON CONFLICT (cpf) DO UPDATE SET nome_colaborador = EXCLUDED.nome_colaborador;\n",
    "\n",
    "        -- (Dimensão Rica)\n",
    "        CREATE TABLE IF NOT EXISTS \"{DB_SCHEMA}\".{NOME_TABELA_RICA} (\n",
    "            colaborador_sk INTEGER PRIMARY KEY, \n",
    "            colaborador_id_solides INTEGER UNIQUE NOT NULL, \n",
    "            cpf VARCHAR(11), \n",
    "            nome_completo VARCHAR(255),\n",
    "            data_nascimento DATE,\n",
    "            genero VARCHAR(50),\n",
    "            data_admissao DATE,\n",
    "            data_demissao DATE,\n",
    "            ativo BOOLEAN,\n",
    "            departamento_nome_api VARCHAR(255),\n",
    "            cargo_nome_api VARCHAR(255),\n",
    "            email VARCHAR(255),\n",
    "            telefone_pessoal VARCHAR(50),\n",
    "            celular VARCHAR(50),\n",
    "            nacionalidade VARCHAR(100),\n",
    "            nivel_educacional VARCHAR(100),\n",
    "            nome_mae VARCHAR(255),\n",
    "            nome_pai VARCHAR(255),\n",
    "            logradouro VARCHAR(255),\n",
    "            numero_endereco VARCHAR(50),\n",
    "            complemento_endereco VARCHAR(100),\n",
    "            bairro VARCHAR(100),\n",
    "            cidade VARCHAR(100),\n",
    "            estado VARCHAR(50),\n",
    "            cep VARCHAR(20),\n",
    "            matricula VARCHAR(50),\n",
    "            estado_civil VARCHAR(50),\n",
    "            salario_api NUMERIC(12, 2),\n",
    "            turno_trabalho VARCHAR(100),\n",
    "            tipo_contrato VARCHAR(100),\n",
    "            curso_formacao VARCHAR(255),\n",
    "            nivel_hierarquico VARCHAR(100),\n",
    "            nome_lider_imediato VARCHAR(255),\n",
    "            etnia VARCHAR(50),\n",
    "            unidade_nome VARCHAR(255),\n",
    "            data_ultima_atualizacao TIMESTAMP DEFAULT current_timestamp,\n",
    "            FOREIGN KEY (colaborador_sk) REFERENCES \"{DB_SCHEMA}\".{NOME_TABELA_BASE}(colaborador_sk)\n",
    "        );\n",
    "\n",
    "        -- *** CORREÇÃO: ADICIONANDO AS COLUNAS NOVAS CASO NÃO EXISTAM ***\n",
    "        ALTER TABLE \"{DB_SCHEMA}\".{NOME_TABELA_RICA}\n",
    "            ADD COLUMN IF NOT EXISTS total_benefits_api NUMERIC(12,2),\n",
    "            ADD COLUMN IF NOT EXISTS total_beneficios_api NUMERIC(12, 2),\n",
    "            ADD COLUMN IF NOT EXISTS data_ultima_atualizacao_api DATE,\n",
    "            ADD COLUMN IF NOT EXISTS cargo_id_solides INTEGER,\n",
    "            ADD COLUMN IF NOT EXISTS departamento_id_solides INTEGER,\n",
    "            -- Colunas novas que causaram o erro:\n",
    "            ADD COLUMN IF NOT EXISTS saudacao VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS tipo_necessidade_especial VARCHAR(100),\n",
    "            ADD COLUMN IF NOT EXISTS local_nascimento VARCHAR(100),\n",
    "            ADD COLUMN IF NOT EXISTS pcd BOOLEAN,\n",
    "            ADD COLUMN IF NOT EXISTS telefone_emergencia VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS email_pessoal VARCHAR(255),\n",
    "            ADD COLUMN IF NOT EXISTS email_corporativo_sec VARCHAR(255),\n",
    "            ADD COLUMN IF NOT EXISTS rg VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS data_emissao_rg DATE,\n",
    "            ADD COLUMN IF NOT EXISTS orgao_emissor_rg VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS titulo_eleitor VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS zona_eleitoral VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS secao_eleitoral VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS ctps_numero VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS ctps_serie VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS pis VARCHAR(50),\n",
    "            ADD COLUMN IF NOT EXISTS motivo_demissao_api VARCHAR(255),\n",
    "            ADD COLUMN IF NOT EXISTS data_contrato DATE,\n",
    "            ADD COLUMN IF NOT EXISTS duracao_contrato VARCHAR(100),\n",
    "            ADD COLUMN IF NOT EXISTS data_expiracao_contrato DATE,\n",
    "            ADD COLUMN IF NOT EXISTS periodo_experiencia_dias INTEGER,\n",
    "            ADD COLUMN IF NOT EXISTS forma_demissao VARCHAR(100),\n",
    "            ADD COLUMN IF NOT EXISTS decisao_demissao VARCHAR(100),\n",
    "            ADD COLUMN IF NOT EXISTS valor_rescisao NUMERIC(12, 2);\n",
    "\n",
    "        INSERT INTO \"{DB_SCHEMA}\".{NOME_TABELA_RICA} (colaborador_sk, colaborador_id_solides)\n",
    "        VALUES (0, -1) ON CONFLICT (colaborador_sk) DO NOTHING;\n",
    "\n",
    "        -- UPSERT DIMENSÃO RICA\n",
    "        INSERT INTO \"{DB_SCHEMA}\".{NOME_TABELA_RICA} (\n",
    "            colaborador_sk, colaborador_id_solides, cpf, nome_completo, data_nascimento, genero,\n",
    "            nacionalidade, nivel_educacional, nome_mae, nome_pai, estado_civil, etnia,\n",
    "            data_admissao, data_demissao, ativo, departamento_nome_api, cargo_nome_api,\n",
    "            matricula, salario_api, turno_trabalho, tipo_contrato, curso_formacao,\n",
    "            nivel_hierarquico, nome_lider_imediato, unidade_nome, email, telefone_pessoal,\n",
    "            celular, logradouro, numero_endereco, complemento_endereco, bairro, cidade,\n",
    "            estado, cep, saudacao, tipo_necessidade_especial, local_nascimento, pcd,\n",
    "            telefone_emergencia, email_pessoal, email_corporativo_sec, rg, data_emissao_rg,\n",
    "            orgao_emissor_rg, titulo_eleitor, zona_eleitoral, secao_eleitoral, ctps_numero,\n",
    "            ctps_serie, pis, motivo_demissao_api, data_contrato, duracao_contrato,\n",
    "            data_expiracao_contrato, periodo_experiencia_dias, forma_demissao, decisao_demissao,\n",
    "            valor_rescisao, total_beneficios_api, cargo_id_solides, departamento_id_solides,\n",
    "            data_ultima_atualizacao_api, data_ultima_atualizacao\n",
    "        )\n",
    "        SELECT\n",
    "            base.colaborador_sk, stg.colaborador_id_solides, stg.cpf, stg.nome_completo, stg.data_nascimento, stg.genero,\n",
    "            stg.nacionalidade, stg.nivel_educacional, stg.nome_mae, stg.nome_pai, stg.estado_civil, stg.etnia,\n",
    "            stg.data_admissao, stg.data_demissao, stg.ativo, stg.departamento_nome_api, stg.cargo_nome_api,\n",
    "            stg.matricula, stg.salario_api, stg.turno_trabalho, stg.tipo_contrato, stg.curso_formacao,\n",
    "            stg.nivel_hierarquico, stg.nome_lider_imediato, stg.unidade_nome, stg.email, stg.telefone_pessoal,\n",
    "            stg.celular, stg.logradouro, stg.numero_endereco, stg.complemento_endereco, stg.bairro, stg.cidade,\n",
    "            stg.estado, stg.cep, stg.saudacao, stg.tipo_necessidade_especial, stg.local_nascimento, stg.pcd,\n",
    "            stg.telefone_emergencia, stg.email_pessoal, stg.email_corporativo_sec, stg.rg, stg.data_emissao_rg,\n",
    "            stg.orgao_emissor_rg, stg.titulo_eleitor, stg.zona_eleitoral, stg.secao_eleitoral, stg.ctps_numero,\n",
    "            stg.ctps_serie, stg.pis, stg.motivo_demissao_api, stg.data_contrato, stg.duracao_contrato,\n",
    "            stg.data_expiracao_contrato, stg.periodo_experiencia_dias, stg.forma_demissao, stg.decisao_demissao,\n",
    "            stg.valor_rescisao, stg.total_beneficios_api, stg.cargo_id_solides, stg.departamento_id_solides,\n",
    "            stg.data_ultima_atualizacao_api, current_timestamp\n",
    "        FROM \"{DB_SCHEMA}\".{NOME_TABELA_STAGING} AS stg\n",
    "        JOIN \"{DB_SCHEMA}\".{NOME_TABELA_BASE} AS base ON stg.cpf = base.cpf\n",
    "        WHERE stg.colaborador_id_solides IS NOT NULL\n",
    "        ON CONFLICT (colaborador_id_solides) DO UPDATE SET\n",
    "            cpf = EXCLUDED.cpf, nome_completo = EXCLUDED.nome_completo, data_nascimento = EXCLUDED.data_nascimento,\n",
    "            genero = EXCLUDED.genero, nacionalidade = EXCLUDED.nacionalidade, nivel_educacional = EXCLUDED.nivel_educacional,\n",
    "            nome_mae = EXCLUDED.nome_mae, nome_pai = EXCLUDED.nome_pai, estado_civil = EXCLUDED.estado_civil,\n",
    "            etnia = EXCLUDED.etnia, data_admissao = EXCLUDED.data_admissao, data_demissao = EXCLUDED.data_demissao,\n",
    "            ativo = EXCLUDED.ativo, departamento_nome_api = EXCLUDED.departamento_nome_api, cargo_nome_api = EXCLUDED.cargo_nome_api,\n",
    "            matricula = EXCLUDED.matricula, salario_api = EXCLUDED.salario_api, turno_trabalho = EXCLUDED.turno_trabalho,\n",
    "            tipo_contrato = EXCLUDED.tipo_contrato, curso_formacao = EXCLUDED.curso_formacao, nivel_hierarquico = EXCLUDED.nivel_hierarquico,\n",
    "            nome_lider_imediato = EXCLUDED.nome_lider_imediato, unidade_nome = EXCLUDED.unidade_nome, email = EXCLUDED.email,\n",
    "            telefone_pessoal = EXCLUDED.telefone_pessoal, celular = EXCLUDED.celular, logradouro = EXCLUDED.logradouro,\n",
    "            numero_endereco = EXCLUDED.numero_endereco, complemento_endereco = EXCLUDED.complemento_endereco, bairro = EXCLUDED.bairro,\n",
    "            cidade = EXCLUDED.cidade, estado = EXCLUDED.estado, cep = EXCLUDED.cep, saudacao = EXCLUDED.saudacao,\n",
    "            tipo_necessidade_especial = EXCLUDED.tipo_necessidade_especial, local_nascimento = EXCLUDED.local_nascimento,\n",
    "            pcd = EXCLUDED.pcd, telefone_emergencia = EXCLUDED.telefone_emergencia, email_pessoal = EXCLUDED.email_pessoal,\n",
    "            email_corporativo_sec = EXCLUDED.email_corporativo_sec, rg = EXCLUDED.rg, data_emissao_rg = EXCLUDED.data_emissao_rg,\n",
    "            orgao_emissor_rg = EXCLUDED.orgao_emissor_rg, titulo_eleitor = EXCLUDED.titulo_eleitor, zona_eleitoral = EXCLUDED.zona_eleitoral,\n",
    "            secao_eleitoral = EXCLUDED.secao_eleitoral, ctps_numero = EXCLUDED.ctps_numero, ctps_serie = EXCLUDED.ctps_serie,\n",
    "            pis = EXCLUDED.pis, motivo_demissao_api = EXCLUDED.motivo_demissao_api, data_contrato = EXCLUDED.data_contrato,\n",
    "            duracao_contrato = EXCLUDED.duracao_contrato, data_expiracao_contrato = EXCLUDED.data_expiracao_contrato,\n",
    "            periodo_experiencia_dias = EXCLUDED.periodo_experiencia_dias, forma_demissao = EXCLUDED.forma_demissao,\n",
    "            decisao_demissao = EXCLUDED.decisao_demissao, valor_rescisao = EXCLUDED.valor_rescisao,\n",
    "            total_beneficios_api = EXCLUDED.total_beneficios_api, cargo_id_solides = EXCLUDED.cargo_id_solides,\n",
    "            departamento_id_solides = EXCLUDED.departamento_id_solides, data_ultima_atualizacao_api = EXCLUDED.data_ultima_atualizacao_api,\n",
    "            data_ultima_atualizacao = current_timestamp;\n",
    "\n",
    "        -- 3.4 FATO BENEFICIOS\n",
    "        CREATE TABLE IF NOT EXISTS \"{DB_SCHEMA}\".{NOME_FATO_BEN} (\n",
    "            beneficio_id SERIAL PRIMARY KEY,\n",
    "            colaborador_sk INTEGER,\n",
    "            tipo_beneficio VARCHAR(100),\n",
    "            nome_beneficio VARCHAR(255),\n",
    "            valor_beneficio NUMERIC(12,2),\n",
    "            valor_desconto NUMERIC(12,2),\n",
    "            periodicidade VARCHAR(50),\n",
    "            opcao_desconto VARCHAR(50),\n",
    "            aplicado_como VARCHAR(50),\n",
    "            data_atualizacao TIMESTAMP DEFAULT current_timestamp,\n",
    "            FOREIGN KEY (colaborador_sk) REFERENCES \"{DB_SCHEMA}\".{NOME_TABELA_BASE}(colaborador_sk)\n",
    "        );\n",
    "        \n",
    "        TRUNCATE TABLE \"{DB_SCHEMA}\".{NOME_FATO_BEN};\n",
    "        \n",
    "        INSERT INTO \"{DB_SCHEMA}\".{NOME_FATO_BEN} (\n",
    "            colaborador_sk, tipo_beneficio, nome_beneficio, \n",
    "            valor_beneficio, valor_desconto, periodicidade, \n",
    "            opcao_desconto, aplicado_como\n",
    "        )\n",
    "        SELECT \n",
    "            base.colaborador_sk,\n",
    "            stg.tipo_beneficio,\n",
    "            stg.nome_beneficio,\n",
    "            stg.valor_beneficio,\n",
    "            stg.valor_desconto,\n",
    "            stg.periodicidade,\n",
    "            stg.opcao_desconto,\n",
    "            stg.aplicado_como\n",
    "        FROM \"{DB_SCHEMA}\".{NOME_STAGING_BEN} stg\n",
    "        JOIN \"{DB_SCHEMA}\".{NOME_TABELA_STAGING} stg_colab ON stg.colaborador_id_solides = stg_colab.colaborador_id_solides\n",
    "        JOIN \"{DB_SCHEMA}\".{NOME_TABELA_BASE} base ON stg_colab.cpf = base.cpf;\n",
    "        \"\"\"\n",
    "        \n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(sql))\n",
    "\n",
    "        print(f\"Carga na {NOME_TABELA_BASE}, {NOME_TABELA_RICA} e {NOME_FATO_BEN} concluída com sucesso!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro na carga de {NOME_TABELA_RICA} ou Beneficios: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- NOVA DIMENSÃO: CALENDÁRIO ---\n",
    "def pipeline_dim_calendario():\n",
    "    \"\"\"Gera ou atualiza a dimensão de calendário (dim_calendario).\"\"\"\n",
    "    print(\"\\n--- Iniciando Pipeline: dim_calendario ---\")\n",
    "    \n",
    "    NOME_TABELA_FINAL = \"dim_calendario\"\n",
    "    \n",
    "    # (Lógica idêntica, sem alterações)\n",
    "    sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS \"{DB_SCHEMA}\".{NOME_TABELA_FINAL} (\n",
    "        data DATE PRIMARY KEY,\n",
    "        ano INTEGER,\n",
    "        mes INTEGER,\n",
    "        dia INTEGER,\n",
    "        trimestre INTEGER,\n",
    "        semestre INTEGER,\n",
    "        dia_da_semana INTEGER, \n",
    "        nome_dia_da_semana VARCHAR(20),\n",
    "        nome_mes VARCHAR(20),\n",
    "        nome_mes_abrev CHAR(3),\n",
    "        ano_mes VARCHAR(7), \n",
    "        dia_do_ano INTEGER,\n",
    "        semana_do_ano INTEGER\n",
    "    );\n",
    "    DO $$\n",
    "    DECLARE\n",
    "        data_inicio DATE := '2023-01-01'; \n",
    "        data_fim DATE := '2030-12-31';\n",
    "    BEGIN\n",
    "        BEGIN\n",
    "            SET LOCAL lc_time = 'pt_BR.UTF-8';\n",
    "        EXCEPTION WHEN OTHERS THEN\n",
    "            BEGIN\n",
    "                SET LOCAL lc_time = 'pt_BR';\n",
    "            EXCEPTION WHEN OTHERS THEN\n",
    "                RAISE NOTICE 'Não foi possível definir o locale pt_BR. Nomes de mês/dia podem ficar em inglês.';\n",
    "            END;\n",
    "        END;\n",
    "        INSERT INTO \"{DB_SCHEMA}\".{NOME_TABELA_FINAL} (\n",
    "            data,\n",
    "            ano, mes, dia, trimestre, semestre,\n",
    "            dia_da_semana, nome_dia_da_semana, nome_mes, nome_mes_abrev,\n",
    "            ano_mes, dia_do_ano, semana_do_ano\n",
    "        )\n",
    "        SELECT\n",
    "            d AS data,\n",
    "            EXTRACT(YEAR FROM d) AS ano,\n",
    "            EXTRACT(MONTH FROM d) AS mes,\n",
    "            EXTRACT(DAY FROM d) AS dia,\n",
    "            EXTRACT(QUARTER FROM d) AS trimestre,\n",
    "            CASE WHEN EXTRACT(MONTH FROM d) <= 6 THEN 1 ELSE 2 END AS semestre,\n",
    "            EXTRACT(DOW FROM d) AS dia_da_semana, \n",
    "            to_char(d, 'TMDay') AS nome_dia_da_semana,\n",
    "            to_char(d, 'TMMonth') AS nome_mes,\n",
    "            to_char(d, 'TMMon') AS nome_mes_abrev,\n",
    "            to_char(d, 'YYYY-MM') AS ano_mes,\n",
    "            EXTRACT(DOY FROM d) AS dia_do_ano,\n",
    "            EXTRACT(WEEK FROM d) AS semana_do_ano\n",
    "        FROM generate_series(data_inicio, data_fim, '1 day'::interval) d\n",
    "        ON CONFLICT (data) DO NOTHING; \n",
    "    END $$;\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(sql))\n",
    "        print(f\"Carga na {NOME_TABELA_FINAL} concluída com sucesso!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Erro na carga de {NOME_TABELA_FINAL}: {e}\")\n",
    "        print(f\"Detalhe do erro: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# --- FASE 2: PIPELINES DAS FATOS (CSV) ---\n",
    "\n",
    "# --- (FUNÇÕES HELPER DE CSV - IDÊNTICAS) ---\n",
    "\n",
    "def clean_text(series):\n",
    "    \"\"\"Limpa uma série de texto (object) de forma segura.\"\"\"\n",
    "    if series.dtype == 'object':\n",
    "        series = series.str.strip()\n",
    "        series = series.str.replace(u'\\xa0', '', regex=False)\n",
    "        series = series.replace(['N/A', '', 'nan', 'None', 'NULL'], None) # Adicionado 'NULL'\n",
    "    return series\n",
    "\n",
    "def para_float(valor_str):\n",
    "    \"\"\"Converte uma string (já limpa) para float.\"\"\"\n",
    "    if valor_str is None or pd.isna(valor_str):\n",
    "        return np.nan # Use numpy's NaN para floats\n",
    "    try:\n",
    "        # CSV do Notebook 1 salva com PONTO decimal (ex: \"1234.56\")\n",
    "        return float(valor_str) \n",
    "    except (ValueError, TypeError):\n",
    "        return np.nan\n",
    "\n",
    "def tratar_tipos_dataframe_csv(df, nome_arquivo):\n",
    "    \"\"\"\n",
    "    Função de tratamento de tipos para os CSVs da FOPAG.\n",
    "    *** VERSÃO CORRIGIDA PARA DATA E FLOAT ***\n",
    "    \"\"\"\n",
    "    print(f\"Iniciando tratamento de tipos para {nome_arquivo}...\")\n",
    "\n",
    "    # --- [CORREÇÃO DATAS] ---\n",
    "    colunas_data = ['competencia', 'data_admissao', 'data_demissao']\n",
    "    for col in colunas_data:\n",
    "        if col in df.columns:\n",
    "            print(f\"Tratando tipo de data: {col}\")\n",
    "            df[col] = clean_text(df[col])\n",
    "            # O CSV já está em formato ISO (YYYY-MM-DD), o pandas lê automaticamente\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce') \n",
    "            df[col] = df[col].dt.date # Converte para objeto date (YYYY-MM-DD)\n",
    "\n",
    "    # --- [CORREÇÃO NUMÉRICOS PARA FLOAT] ---\n",
    "    colunas_monetarias = [\n",
    "        'salario_contratual', 'total_proventos', 'total_descontos',\n",
    "        'valor_liquido', 'base_inss', 'base_fgts', 'valor_fgts',\n",
    "        'base_irrf', 'valor_rubrica'\n",
    "    ]\n",
    "    for col in colunas_monetarias:\n",
    "        if col in df.columns:\n",
    "            print(f\"Tratando tipo: {col} (String -> Float)\")\n",
    "            df[col] = clean_text(df[col]) \n",
    "            # Usa a nova função para_float\n",
    "            df[col] = df[col].apply(para_float)\n",
    "            # Garante que a coluna inteira seja do tipo float no Pandas\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce') \n",
    "\n",
    "    # --- [CPF E TEXTO] ---\n",
    "    if 'cpf' in df.columns:\n",
    "        print(\"Tratando tipo: cpf (String -> String Limpa)\")\n",
    "        df['cpf'] = clean_text(df['cpf'])\n",
    "        df['cpf'] = df['cpf'].str.replace(r'[^\\d]', '', regex=True)\n",
    "\n",
    "    # Adiciona os novos campos de FATO à limpeza de texto\n",
    "    colunas_texto = [\n",
    "        'departamento', 'vinculo', 'nome_funcionario',\n",
    "        'motivo_demissao', 'cargo', 'codigo_rubrica',\n",
    "        'nome_rubrica', 'tipo_rubrica',\n",
    "        \n",
    "        # NOVOS CAMPOS PARA A FATO\n",
    "        'situacao', \n",
    "        'tipo_calculo' \n",
    "    ]\n",
    "    \n",
    "    for col in colunas_texto:\n",
    "        if col in df.columns:\n",
    "            df[col] = clean_text(df[col])\n",
    "\n",
    "    print(\"Tratamento de tipos finalizado.\")\n",
    "    return df\n",
    "# --- [FIM DAS FUNÇÕES HELPER DE CSV CORRIGIDAS] ---\n",
    "\n",
    "\n",
    "def pipeline_fato_folha_consolidada():\n",
    "    print(\"\\n--- Iniciando Pipeline: fato_folha_consolidada ---\")\n",
    "\n",
    "    CSV_FILE = 'BASE_FOPAG_CONSOLIDADA_TOTAIS.csv'\n",
    "    NOME_TABELA_STAGING = 'staging_folha_consolidada'\n",
    "    NOME_TABELA_FINAL = 'fato_folha_consolidada'\n",
    "    NOME_TABELA_BASE_COLAB = 'dim_colaboradores_base' \n",
    "\n",
    "    try:\n",
    "        # 1. Extract\n",
    "        try:\n",
    "            df_csv = pd.read_csv(CSV_FILE, sep=';', dtype=str) # Lê tudo como string\n",
    "        except FileNotFoundError:\n",
    "             print(f\"ERRO: Arquivo '{CSV_FILE}' não encontrado.\")\n",
    "             print(\"Por favor, execute o Notebook 1 (Automação_FOPAG.ipynb) para gerar os CSVs primeiro.\")\n",
    "             return False\n",
    "        except Exception as read_err:\n",
    "            print(f\"Erro ao ler CSV {CSV_FILE}: {read_err}.\")\n",
    "            return False\n",
    "\n",
    "        # 2. Transformação (T)\n",
    "        df_tratado = tratar_tipos_dataframe_csv(df_csv.copy(), CSV_FILE)\n",
    "\n",
    "        # --- [INÍCIO DA ATUALIZAÇÃO] ---\n",
    "        # Popula a dim_colaboradores_base com os dados MESTRE do CSV\n",
    "        print(f\"Extraindo dados mestre do '{CSV_FILE}' para popular a dim_colaboradores_base...\")\n",
    "        if 'cpf' in df_tratado.columns:\n",
    "            # Ordena para pegar os dados mais recentes de cada CPF (baseado na competência)\n",
    "            df_recentes = df_tratado.sort_values(by='competencia', ascending=False).drop_duplicates(subset=['cpf'])\n",
    "            \n",
    "            # Seleciona as colunas mestre\n",
    "            colunas_mestre = [\n",
    "                'cpf', \n",
    "                'nome_funcionario', \n",
    "                'data_admissao', \n",
    "                'data_demissao', \n",
    "                'situacao', \n",
    "                'departamento', # 'departamento' do CSV\n",
    "                'cargo'         # 'cargo' do CSV\n",
    "            ]\n",
    "            \n",
    "            # Garante que todas as colunas existem no DF\n",
    "            colunas_presentes = [col for col in colunas_mestre if col in df_recentes.columns]\n",
    "            df_colabs_unicos = df_recentes[colunas_presentes].copy()\n",
    "            \n",
    "            # Renomeia para o padrão da função helper\n",
    "            df_colabs_unicos = df_colabs_unicos.rename(columns={\n",
    "                'nome_funcionario': 'nome_colaborador',\n",
    "                'data_admissao': 'data_admissao_csv',\n",
    "                'data_demissao': 'data_demissao_csv',\n",
    "                'situacao': 'situacao_csv',\n",
    "                'departamento': 'departamento_csv',\n",
    "                'cargo': 'cargo_csv'\n",
    "            })\n",
    "            \n",
    "            # Garante que a função helper receba colunas mesmo se não existirem no CSV\n",
    "            colunas_helper_esperadas = [\n",
    "                'nome_colaborador', 'cpf', 'data_admissao_csv', 'data_demissao_csv',\n",
    "                'situacao_csv', 'departamento_csv', 'cargo_csv'\n",
    "            ]\n",
    "            for col in colunas_helper_esperadas:\n",
    "                if col not in df_colabs_unicos.columns:\n",
    "                    df_colabs_unicos[col] = None\n",
    "\n",
    "            # --- [INÍCIO DA CORREÇÃO 3/3 - Parte 3] ---\n",
    "            # Captura o retorno da função helper\n",
    "            sucesso_upsert = atualizar_dim_colaboradores_base(engine, df_colabs_unicos, DB_SCHEMA)\n",
    "            \n",
    "            if not sucesso_upsert:\n",
    "                print(\"Falha ao atualizar a dim_colaboradores_base. Abortando pipeline da Fato Consolidada.\")\n",
    "                return False # Para a execução\n",
    "            # --- [FIM DA CORREÇÃO 3/3 - Parte 3] ---\n",
    "\n",
    "        else:\n",
    "            print(\"AVISO: Coluna 'cpf' não encontrada no CSV. Não foi possível popular a dim_colaboradores_base.\")\n",
    "        # --- [FIM DA ATUALIZAÇÃO] ---\n",
    "\n",
    "\n",
    "        # 3. Carga (L)\n",
    "        \n",
    "        # Agora o df_tratado tem colunas float, o to_sql vai criar a staging table\n",
    "        # com os tipos numéricos corretos, sem precisar do dtype_map.\n",
    "        print(f\"Carregando CSV para {NOME_TABELA_STAGING}...\")\n",
    "        df_tratado.to_sql(\n",
    "            NOME_TABELA_STAGING, \n",
    "            engine, \n",
    "            if_exists='replace', \n",
    "            index=False, \n",
    "            schema=DB_SCHEMA\n",
    "        )\n",
    "        print(f\"CSV carregado para {NOME_TABELA_STAGING}.\")\n",
    "        \n",
    "        \n",
    "        competencias_no_df = df_tratado['competencia'].dropna().unique()\n",
    "        if len(competencias_no_df) == 0:\n",
    "            print(\"ERRO CRÍTICO: Nenhuma competência válida encontrada no CSV após o tratamento.\")\n",
    "            return False \n",
    "        \n",
    "        print(f\"Competências a serem carregadas na Fato: {len(competencias_no_df)} meses/períodos.\")\n",
    "\n",
    "        # --- (SQL para popular a Fato - IDÊNTICO) ---\n",
    "        sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS \"{DB_SCHEMA}\".{NOME_TABELA_FINAL} (\n",
    "            fato_folha_id SERIAL PRIMARY KEY,\n",
    "            colaborador_sk INTEGER, \n",
    "            competencia DATE,\n",
    "            nome_funcionario_csv VARCHAR(255), \n",
    "            centro_de_custo VARCHAR(255), \n",
    "            cargo_nome_csv VARCHAR(255),  \n",
    "            cpf_csv VARCHAR(11),\n",
    "            \n",
    "            -- Novos campos de FATO\n",
    "            situacao_csv VARCHAR(100),\n",
    "            tipo_calculo_csv VARCHAR(100),\n",
    "            \n",
    "            -- Métricas\n",
    "            salario_contratual NUMERIC(12, 2),\n",
    "            total_proventos NUMERIC(12, 2),\n",
    "            total_descontos NUMERIC(12, 2),\n",
    "            valor_liquido NUMERIC(12, 2),\n",
    "            base_inss NUMERIC(12, 2),\n",
    "            base_fgts NUMERIC(12, 2),\n",
    "            valor_fgts NUMERIC(12, 2),\n",
    "            base_irrf NUMERIC(12, 2),\n",
    "            FOREIGN KEY (colaborador_sk) REFERENCES \"{DB_SCHEMA}\".{NOME_TABELA_BASE_COLAB}(colaborador_sk)\n",
    "        );\n",
    "\n",
    "        DELETE FROM \"{DB_SCHEMA}\".{NOME_TABELA_FINAL}\n",
    "        WHERE competencia IN :competencias_list;\n",
    "\n",
    "        INSERT INTO \"{DB_SCHEMA}\".{NOME_TABELA_FINAL} (\n",
    "            colaborador_sk, \n",
    "            competencia,\n",
    "            nome_funcionario_csv, centro_de_custo, cargo_nome_csv, cpf_csv,\n",
    "            situacao_csv, tipo_calculo_csv, -- NOVOS\n",
    "            salario_contratual, total_proventos, total_descontos, valor_liquido,\n",
    "            base_inss, base_fgts, valor_fgts, base_irrf\n",
    "        )\n",
    "        SELECT\n",
    "            COALESCE(base.colaborador_sk, 0) AS colaborador_sk,\n",
    "            stg.competencia,\n",
    "            stg.nome_funcionario AS nome_funcionario_csv, \n",
    "            stg.departamento AS centro_de_custo, \n",
    "            stg.cargo AS cargo_nome_csv,        \n",
    "            stg.cpf AS cpf_csv,               \n",
    "\n",
    "            stg.situacao AS situacao_csv,       -- NOVO\n",
    "            stg.tipo_calculo AS tipo_calculo_csv, -- NOVO\n",
    "\n",
    "            stg.salario_contratual, stg.total_proventos, stg.total_descontos, stg.valor_liquido,\n",
    "            stg.base_inss, stg.base_fgts, stg.valor_fgts, stg.base_irrf\n",
    "        FROM\n",
    "            \"{DB_SCHEMA}\".{NOME_TABELA_STAGING} AS stg\n",
    "        LEFT JOIN\n",
    "            \"{DB_SCHEMA}\".{NOME_TABELA_BASE_COLAB} AS base ON stg.cpf = base.cpf\n",
    "        ;\n",
    "        \"\"\"\n",
    "        \n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(sql), {\"competencias_list\": tuple(competencias_no_df)})\n",
    "            \n",
    "        print(f\"Carga na {NOME_TABELA_FINAL} concluída com sucesso!\")\n",
    "        return True\n",
    "\n",
    "    except sqlalchemy_exc.SQLAlchemyError as e: \n",
    "        print(f\"Falha no pipeline {NOME_TABELA_FINAL} (SQLAlchemyError): {e}\")\n",
    "        if hasattr(e, 'orig') and e.orig:\n",
    "             print(f\"  Erro original (psycopg2): {e.orig}\")\n",
    "        return False\n",
    "    except pd.errors.ParserError as e: \n",
    "       print(f\"Falha ao ler o CSV {CSV_FILE}: {e}\")\n",
    "       return False\n",
    "    except Exception as e: \n",
    "        print(f\"Falha no pipeline {NOME_TABELA_FINAL} (Erro genérico): {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def pipeline_fato_folha_detalhada():\n",
    "    print(\"\\n--- Iniciando Pipeline: fato_folha_detalhada ---\")\n",
    "\n",
    "    CSV_FILE = 'BASE_FOPAG_DETALHADA_RUBRICAS.csv'\n",
    "    NOME_TABELA_STAGING = 'staging_folha_detalhada'\n",
    "    NOME_TABELA_FINAL = 'fato_folha_detalhada'\n",
    "    NOME_TABELA_BASE_COLAB = 'dim_colaboradores_base' \n",
    "\n",
    "    try:\n",
    "        # 1. Extract\n",
    "        try:\n",
    "            df_csv = pd.read_csv(CSV_FILE, sep=';', dtype=str) # Lê tudo como string\n",
    "        except FileNotFoundError:\n",
    "             print(f\"ERRO: Arquivo '{CSV_FILE}' não encontrado.\")\n",
    "             print(\"Por favor, execute o Notebook 1 (Automação_FOPAG.ipynb) para gerar os CSVs primeiro.\")\n",
    "             return False\n",
    "        except Exception as read_err:\n",
    "            print(f\"Erro ao ler CSV {CSV_FILE}: {read_err}.\")\n",
    "            return False\n",
    "\n",
    "        # 2. Transformação (T)\n",
    "        df_tratado = tratar_tipos_dataframe_csv(df_csv.copy(), CSV_FILE)\n",
    "        \n",
    "        # --- [INÍCIO DA CORREÇÃO 2/3] ---\n",
    "        # Garante que as colunas de FATO esperadas existam no DataFrame\n",
    "        # antes de carregar para a staging. Isso corrige o Erro 2.\n",
    "        colunas_fato_esperadas = ['situacao', 'tipo_calculo']\n",
    "        for col in colunas_fato_esperadas:\n",
    "            if col not in df_tratado.columns:\n",
    "                print(f\"Aviso: Coluna '{col}' não encontrada no CSV {CSV_FILE}. Será preenchida com Nulo.\")\n",
    "                df_tratado[col] = None\n",
    "        # --- [FIM DA CORREÇÃO 2/3] ---\n",
    "        \n",
    "        \n",
    "        # 3. Carga (L)\n",
    "        \n",
    "        # Converte para float para o to_sql criar a staging table com tipo numérico\n",
    "        print(f\"Carregando CSV para {NOME_TABELA_STAGING}...\")\n",
    "        df_tratado.to_sql(\n",
    "            NOME_TABELA_STAGING, \n",
    "            engine, \n",
    "            if_exists='replace', \n",
    "            index=False, \n",
    "            schema=DB_SCHEMA\n",
    "        )\n",
    "        print(f\"CSV carregado para {NOME_TABELA_STAGING}.\")\n",
    "        \n",
    "        \n",
    "        competencias_no_df = df_tratado['competencia'].dropna().unique()\n",
    "        if len(competencias_no_df) == 0:\n",
    "            print(\"ERRO CRÍTICO: Nenhuma competência válida encontrada no CSV após o tratamento.\")\n",
    "            return False \n",
    "        \n",
    "        print(f\"Competências a serem carregadas na Fato: {len(competencias_no_df)} meses/períodos.\")\n",
    "\n",
    "        # --- (SQL para popular a Fato - IDÊNTICO) ---\n",
    "        sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS \"{DB_SCHEMA}\".{NOME_TABELA_FINAL} (\n",
    "            fato_rubrica_id SERIAL PRIMARY KEY,\n",
    "            colaborador_sk INTEGER, \n",
    "            competencia DATE,\n",
    "            nome_funcionario_csv VARCHAR(255), \n",
    "            centro_de_custo VARCHAR(255), \n",
    "            cpf_csv VARCHAR(11),\n",
    "            \n",
    "            -- Novos campos de FATO\n",
    "            situacao_csv VARCHAR(100),\n",
    "            tipo_calculo_csv VARCHAR(100),\n",
    "            \n",
    "            -- Detalhes da Rubrica\n",
    "            codigo_rubrica VARCHAR(100),\n",
    "            nome_rubrica VARCHAR(255),\n",
    "            tipo_rubrica VARCHAR(100),\n",
    "            valor_rubrica NUMERIC(12, 2),\n",
    "            FOREIGN KEY (colaborador_sk) REFERENCES \"{DB_SCHEMA}\".{NOME_TABELA_BASE_COLAB}(colaborador_sk)\n",
    "        );\n",
    "\n",
    "        DELETE FROM \"{DB_SCHEMA}\".{NOME_TABELA_FINAL}\n",
    "        WHERE competencia IN :competencias_list;\n",
    "\n",
    "        INSERT INTO \"{DB_SCHEMA}\".{NOME_TABELA_FINAL} (\n",
    "            colaborador_sk, \n",
    "            competencia,\n",
    "            nome_funcionario_csv, centro_de_custo, cpf_csv,\n",
    "            situacao_csv, tipo_calculo_csv, -- NOVOS\n",
    "            codigo_rubrica, nome_rubrica, tipo_rubrica, valor_rubrica\n",
    "        )\n",
    "        SELECT\n",
    "            COALESCE(base.colaborador_sk, 0) AS colaborador_sk,\n",
    "            stg.competencia,\n",
    "            stg.nome_funcionario AS nome_funcionario_csv, \n",
    "            stg.departamento AS centro_de_custo, \n",
    "            stg.cpf AS cpf_csv,               \n",
    "            \n",
    "            stg.situacao AS situacao_csv,       -- NOVO\n",
    "            stg.tipo_calculo AS tipo_calculo_csv, -- NOVO\n",
    "            \n",
    "            stg.codigo_rubrica, stg.nome_rubrica, stg.tipo_rubrica, stg.valor_rubrica\n",
    "        FROM\n",
    "            \"{DB_SCHEMA}\".{NOME_TABELA_STAGING} AS stg\n",
    "        LEFT JOIN\n",
    "            \"{DB_SCHEMA}\".{NOME_TABELA_BASE_COLAB} AS base ON stg.cpf = base.cpf\n",
    "        ;\n",
    "        \"\"\"\n",
    "\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(sql), {\"competencias_list\": tuple(competencias_no_df)})\n",
    "            \n",
    "        print(f\"Carga na {NOME_TABELA_FINAL} concluída com sucesso!\")\n",
    "        return True\n",
    "\n",
    "    except sqlalchemy_exc.SQLAlchemyError as e: \n",
    "        print(f\"Falha no pipeline {NOME_TABELA_FINAL} (SQLAlchemyError): {e}\")\n",
    "        if hasattr(e, 'orig') and e.orig:\n",
    "             print(f\"  Erro original (psycopg2): {e.orig}\")\n",
    "        return False\n",
    "    except pd.errors.ParserError as e: \n",
    "       print(f\"Falha ao ler o CSV {CSV_FILE}: {e}\")\n",
    "       return False\n",
    "    except Exception as e: \n",
    "        print(f\"Falha no pipeline {NOME_TABELA_FINAL} (Erro genérico): {e}\")\n",
    "        return False\n",
    "    \n",
    "\n",
    "def processar_status_transferidos():\n",
    "    \"\"\"\n",
    "    Identifica colaboradores que 'sumiram' da carga (API e CSV) sem data de demissão\n",
    "    e atualiza o status para 'Transferido'.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Iniciando Pós-Processamento: Identificação de Transferidos ---\")\n",
    "    \n",
    "    # Nomes das tabelas\n",
    "    TB_BASE = f'\"{DB_SCHEMA}\".dim_colaboradores_base'\n",
    "    TB_RICA = f'\"{DB_SCHEMA}\".dim_colaboradores'\n",
    "    TB_STG_API = f'\"{DB_SCHEMA}\".staging_colaboradores'\n",
    "    TB_STG_CSV = f'\"{DB_SCHEMA}\".staging_folha_consolidada'\n",
    "\n",
    "    # SQL Lógico:\n",
    "    # 1. Pegar quem está na BASE e NÃO tem data de demissão.\n",
    "    # 2. Verificar se esse CPF NÃO está na Staging da API (carga de hoje).\n",
    "    # 3. Verificar se esse CPF NÃO está na Staging do CSV (carga do mês).\n",
    "    # 4. Se atender a tudo, marca como Transferido.\n",
    "\n",
    "    sql_update = text(f\"\"\"\n",
    "        UPDATE {TB_BASE}\n",
    "        SET \n",
    "            situacao_csv = 'Transferido'\n",
    "        WHERE cpf IN (\n",
    "            -- Seleciona CPFs candidatos a Transferência\n",
    "            SELECT base.cpf\n",
    "            FROM {TB_BASE} base\n",
    "            -- Que NÃO estão na API hoje\n",
    "            LEFT JOIN {TB_STG_API} api ON base.cpf = api.cpf\n",
    "            -- Que NÃO estão no CSV hoje\n",
    "            LEFT JOIN {TB_STG_CSV} csv ON base.cpf = csv.cpf\n",
    "            WHERE \n",
    "                api.cpf IS NULL              -- Sumiu da API\n",
    "                AND csv.cpf IS NULL          -- Sumiu do CSV\n",
    "                AND base.data_demissao_csv IS NULL -- Não foi demitido oficialmente\n",
    "                AND base.situacao_csv != 'Transferido' -- Já não é transferido\n",
    "                AND base.situacao_csv != 'Desligado'   -- Já não é desligado\n",
    "        );\n",
    "    \"\"\")\n",
    "    \n",
    "    # Atualiza também a Dimensão Rica para refletir a mudança\n",
    "    sql_sync_rica = text(f\"\"\"\n",
    "        UPDATE {TB_RICA}\n",
    "        SET \n",
    "            ativo = False, -- Transferido não conta como ativo na unidade atual\n",
    "            data_ultima_atualizacao = current_timestamp\n",
    "        FROM {TB_BASE} base\n",
    "        WHERE {TB_RICA}.colaborador_sk = base.colaborador_sk\n",
    "        AND base.situacao_csv = 'Transferido'\n",
    "        AND {TB_RICA}.ativo = True; -- Só atualiza se ainda constava como ativo\n",
    "    \"\"\")\n",
    "\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            # Executa a marcação na Base\n",
    "            result = conn.execute(sql_update)\n",
    "            afetados = result.rowcount\n",
    "            print(f\"Detectados e marcados como 'Transferido' na Base: {afetados} colaboradores.\")\n",
    "            \n",
    "            # Sincroniza a Rica\n",
    "            if afetados > 0:\n",
    "                result_rica = conn.execute(sql_sync_rica)\n",
    "                print(f\"Status 'Ativo' atualizado para False na Dimensão Rica: {result_rica.rowcount} registros.\")\n",
    "            \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar transferidos: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# --- PONTO DE EXECUÇÃO PRINCIPAL --\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Ordem de execution é crucial\n",
    "\n",
    "    # 1. Dimensões independentes\n",
    "    sucesso_colab = pipeline_dim_colaboradores() # Popula a Base com dados da API\n",
    "    sucesso_calendario = pipeline_dim_calendario() \n",
    "\n",
    "    # 2. Fatos (dependentes)\n",
    "    # Agora, o pipeline da FATO irá *primeiro* popular a Base com dados do CSV\n",
    "    # e *depois* carregar a Fato.\n",
    "    \n",
    "    if sucesso_colab and sucesso_calendario:\n",
    "        # A Consolidada AGORA atualiza a dim_colaboradores_base\n",
    "        sucesso_fato_cons = pipeline_fato_folha_consolidada()\n",
    "        \n",
    "        # A Detalhada apenas lê da dim_colaboradores_base\n",
    "        # Ela só executa se a consolidada (que atualiza a base) funcionar\n",
    "        if sucesso_fato_cons:\n",
    "            sucesso_fato_det = pipeline_fato_folha_detalhada()\n",
    "            # --- NOVO PASSO: RODAR APÓS AS CARGAS ---\n",
    "            processar_status_transferidos()\n",
    "            # ----------------------------------------\n",
    "        else:\n",
    "            sucesso_fato_det = False # Pula a execução\n",
    "        \n",
    "        if not sucesso_fato_cons or not sucesso_fato_det:\n",
    "             print(\"\\n!!! Atenção: Pelo menos um pipeline de FATO falhou. Verifique os logs acima. !!!\")\n",
    "        else:\n",
    "             print(\"\\n--- Pipeline ETL Concluído com Sucesso! ---\")\n",
    "\n",
    "    else:\n",
    "        if not sucesso_colab:\n",
    "             print(\"\\nFalha ao carregar dim_colaboradores (API). Abortando pipelines de Fatos.\")\n",
    "        if not sucesso_calendario:\n",
    "             print(\"\\nFalha ao carregar dim_calendario. Abortando pipelines de Fatos.\")\n",
    "        sys.exit() # Encerra se as dimensões falharem\n",
    "\n",
    "\n",
    "    # Fecha a conexão com o banco\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "082c7c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Diagnóstico de Benefícios ---\n",
      "Verificando 20 colaboradores...\n",
      "[ACHEI!] ID 3382492 (Acassio Moraes Forasteiro de Souza)\n",
      "   - Total: 2616.97\n",
      "   - Lista: [{'benefitName': 'Sólides Alimentação', 'typeBenefit': 'Vale Alimentação', 'value': 'R$ 500,00', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Sólides Refeição', 'typeBenefit': 'Vale Refeição', 'value': 'R$ 1.122,00', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 1,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Tokio Marine - Gestor', 'typeBenefit': 'Seguro de Vida', 'value': 'R$ 23,34', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'SulAmérica Odonto - Mais Doc', 'typeBenefit': 'Plano Odontológico', 'value': 'R$ 13,22', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Bradesco Saúde Nacional Top (34-38 Anos)', 'typeBenefit': 'Plano de Saúde', 'value': 'R$ 923,65', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Vale Cultura (Happy Day)', 'typeBenefit': 'Outros', 'value': 'R$ 4,16', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Beneficio Social Familiar', 'typeBenefit': 'Outros', 'value': 'R$ 30,60', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}]\n",
      "[ACHEI!] ID 2050250 (Anderson César Moura Gonçalves)\n",
      "   - Total: 2384.33\n",
      "   - Lista: [{'benefitName': 'Sólides Alimentação', 'typeBenefit': 'Vale Alimentação', 'value': 'R$ 500,00', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Sólides Refeição', 'typeBenefit': 'Vale Refeição', 'value': 'R$ 1.122,00', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 1,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Tokio Marine - Colaborador', 'typeBenefit': 'Seguro de Vida', 'value': 'R$ 4,55', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'SulAmérica Odonto - Mais Clarear', 'typeBenefit': 'Plano Odontológico', 'value': 'R$ 34,79', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Bradesco Saúde Nacional Top (24-28 Anos)', 'typeBenefit': 'Plano de Saúde', 'value': 'R$ 688,23', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Vale Cultura (Happy Day)', 'typeBenefit': 'Outros', 'value': 'R$ 4,16', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Beneficio Social Familiar', 'typeBenefit': 'Outros', 'value': 'R$ 30,60', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}]\n",
      "[ACHEI!] ID 2284524 (Anderson Marques de Oliveira)\n",
      "   - Total: 2619.75\n",
      "   - Lista: [{'benefitName': 'Sólides Alimentação', 'typeBenefit': 'Vale Alimentação', 'value': 'R$ 500,00', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Sólides Refeição', 'typeBenefit': 'Vale Refeição', 'value': 'R$ 1.122,00', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 1,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Tokio Marine - Colaborador', 'typeBenefit': 'Seguro de Vida', 'value': 'R$ 4,55', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'SulAmérica Odonto - Mais Clarear', 'typeBenefit': 'Plano Odontológico', 'value': 'R$ 34,79', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Bradesco Saúde Nacional Top (34-38 Anos)', 'typeBenefit': 'Plano de Saúde', 'value': 'R$ 923,65', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Vale Cultura (Happy Day)', 'typeBenefit': 'Outros', 'value': 'R$ 4,16', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Beneficio Social Familiar', 'typeBenefit': 'Outros', 'value': 'R$ 30,60', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}]\n",
      "[ACHEI!] ID 3580987 (Andrey dos Reis Silva)\n",
      "   - Total: 2535.77\n",
      "   - Lista: [{'benefitName': 'Sólides Alimentação', 'typeBenefit': 'Vale Alimentação', 'value': 'R$ 500,00', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Sólides Refeição', 'typeBenefit': 'Vale Refeição', 'value': 'R$ 1.122,00', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 1,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Tokio Marine - Colaborador', 'typeBenefit': 'Seguro de Vida', 'value': 'R$ 4,55', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'SulAmérica Odonto - Mais Clarear', 'typeBenefit': 'Plano Odontológico', 'value': 'R$ 34,79', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Bradesco Saúde Nacional Top (29-33 Anos)', 'typeBenefit': 'Plano de Saúde', 'value': 'R$ 839,67', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Vale Cultura (Happy Day)', 'typeBenefit': 'Outros', 'value': 'R$ 4,16', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Beneficio Social Familiar', 'typeBenefit': 'Outros', 'value': 'R$ 30,60', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}]\n",
      "[ACHEI!] ID 2566389 (Anna Virgínia Souza Ferreira)\n",
      "   - Total: 2649.94\n",
      "   - Lista: [{'benefitName': 'Sólides Alimentação', 'typeBenefit': 'Vale Alimentação', 'value': 'R$ 500,00', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Sólides Refeição', 'typeBenefit': 'Vale Refeição', 'value': 'R$ 1.122,00', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 1,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Tokio Marine - Gestor', 'typeBenefit': 'Seguro de Vida', 'value': 'R$ 23,34', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Bradesco Saúde Nacional Top (39-43 Anos)', 'typeBenefit': 'Plano de Saúde', 'value': 'R$ 969,84', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Vale Cultura (Happy Day)', 'typeBenefit': 'Outros', 'value': 'R$ 4,16', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Beneficio Social Familiar', 'typeBenefit': 'Outros', 'value': 'R$ 30,60', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}]\n",
      "[ACHEI!] ID 2866923 (Ayrton Antonio Meira Alves)\n",
      "   - Total: 2558.57\n",
      "   - Lista: [{'benefitName': 'Sólides Alimentação', 'typeBenefit': 'Vale Alimentação', 'value': 'R$ 500,00', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Sólides Refeição', 'typeBenefit': 'Vale Refeição', 'value': 'R$ 1.122,00', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 1,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Tokio Marine - Colaborador', 'typeBenefit': 'Seguro de Vida', 'value': 'R$ 4,55', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'SulAmérica Odonto - Mais Orto', 'typeBenefit': 'Plano Odontológico', 'value': 'R$ 57,59', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Bradesco Saúde Nacional Top (29-33 Anos)', 'typeBenefit': 'Plano de Saúde', 'value': 'R$ 839,67', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Vale Cultura (Happy Day)', 'typeBenefit': 'Outros', 'value': 'R$ 4,16', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Beneficio Social Familiar', 'typeBenefit': 'Outros', 'value': 'R$ 30,60', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}]\n",
      "[ACHEI!] ID 3506783 (Brenda Gabriela Santos de Oliveira)\n",
      "   - Total: 2558.57\n",
      "   - Lista: [{'benefitName': 'Sólides Alimentação', 'typeBenefit': 'Vale Alimentação', 'value': 'R$ 500,00', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Sólides Refeição', 'typeBenefit': 'Vale Refeição', 'value': 'R$ 1.122,00', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 1,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Tokio Marine - Colaborador', 'typeBenefit': 'Seguro de Vida', 'value': 'R$ 4,55', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'SulAmérica Odonto - Mais Orto', 'typeBenefit': 'Plano Odontológico', 'value': 'R$ 57,59', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Bradesco Saúde Nacional Top (29-33 Anos)', 'typeBenefit': 'Plano de Saúde', 'value': 'R$ 839,67', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Vale Cultura (Happy Day)', 'typeBenefit': 'Outros', 'value': 'R$ 4,16', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Beneficio Social Familiar', 'typeBenefit': 'Outros', 'value': 'R$ 30,60', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}]\n",
      "[ACHEI!] ID 2274832 (Camila Aparecida de Araujo de Souza)\n",
      "   - Total: 2558.57\n",
      "   - Lista: [{'benefitName': 'Sólides Alimentação', 'typeBenefit': 'Vale Alimentação', 'value': 'R$ 500,00', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Sólides Refeição', 'typeBenefit': 'Vale Refeição', 'value': 'R$ 1.122,00', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 1,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Tokio Marine - Colaborador', 'typeBenefit': 'Seguro de Vida', 'value': 'R$ 4,55', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'SulAmérica Odonto - Mais Orto', 'typeBenefit': 'Plano Odontológico', 'value': 'R$ 57,59', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Bradesco Saúde Nacional Top (29-33 Anos)', 'typeBenefit': 'Plano de Saúde', 'value': 'R$ 839,67', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Vale Cultura (Happy Day)', 'typeBenefit': 'Outros', 'value': 'R$ 4,16', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Beneficio Social Familiar', 'typeBenefit': 'Outros', 'value': 'R$ 30,60', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}]\n",
      "[ACHEI!] ID 805342 (Camila Lins De Anjos)\n",
      "   - Total: 2194.91\n",
      "   - Lista: [{'benefitName': 'Sólides Alimentação', 'typeBenefit': 'Vale Alimentação', 'value': 'R$ 500,00', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Sólides Refeição', 'typeBenefit': 'Vale Refeição', 'value': 'R$ 1.122,00', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 1,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Tokio Marine - Colaborador', 'typeBenefit': 'Seguro de Vida', 'value': 'R$ 4,55', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'SulAmérica Odonto - Mais Clarear', 'typeBenefit': 'Plano Odontológico', 'value': 'R$ 34,79', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Bradesco Saúde Nacional Top (19-23 Anos)', 'typeBenefit': 'Plano de Saúde', 'value': 'R$ 529,41', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Vale Cultura (Happy Day)', 'typeBenefit': 'Outros', 'value': 'R$ 4,16', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}, {'benefitName': 'Beneficio Social Familiar', 'typeBenefit': 'Outros', 'value': 'R$ 30,60', 'benefitAppliedAs': 'em_reais', 'dates': 'mensal', 'valueDiscount': 'R$ 0,00', 'discountOption': 'valor_fixo'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configuração\n",
    "load_dotenv()\n",
    "API_TOKEN = os.getenv('SOLIDES_API_TOKEN')\n",
    "BASE_URL = \"https://app.solides.com/pt-BR/api/v1\"\n",
    "HEADERS = {\"Authorization\": f\"Token token={API_TOKEN}\", \"Accept\": \"application/json\"}\n",
    "\n",
    "print(\"--- Diagnóstico de Benefícios ---\")\n",
    "\n",
    "# 1. Pega uma lista pequena de colaboradores\n",
    "url = f\"{BASE_URL}/colaboradores?page=1&page_size=20&status=todos\"\n",
    "resp = requests.get(url, headers=HEADERS)\n",
    "\n",
    "if resp.status_code != 200:\n",
    "    print(f\"Erro na API: {resp.status_code}\")\n",
    "else:\n",
    "    lista = resp.json()\n",
    "    encontrou_algum = False\n",
    "    \n",
    "    print(f\"Verificando {len(lista)} colaboradores...\")\n",
    "    \n",
    "    for colab in lista:\n",
    "        # Para cada um, busca o detalhe (onde ficam os benefícios)\n",
    "        id_colab = colab.get('id')\n",
    "        resp_detalhe = requests.get(f\"{BASE_URL}/colaboradores/{id_colab}\", headers=HEADERS)\n",
    "        \n",
    "        if resp_detalhe.status_code == 200:\n",
    "            dados = resp_detalhe.json()\n",
    "            \n",
    "            # Verifica os campos\n",
    "            total = dados.get('totalBenefits', 'N/A')\n",
    "            lista_beneficios = dados.get('benefits', [])\n",
    "            \n",
    "            # Se tiver qualquer coisa diferente de zero ou vazio, avisa\n",
    "            if (total != \"0.0\" and total != 0) or (len(lista_beneficios) > 0):\n",
    "                print(f\"[ACHEI!] ID {id_colab} ({dados.get('name')})\")\n",
    "                print(f\"   - Total: {total}\")\n",
    "                print(f\"   - Lista: {lista_beneficios}\")\n",
    "                encontrou_algum = True\n",
    "                # Se quiser parar no primeiro que achar, descomente abaixo:\n",
    "            #break \n",
    "    \n",
    "    if not encontrou_algum:\n",
    "        print(\"\\n[RESULTADO] Nenhum benefício encontrado nos primeiros 20 colaboradores.\")\n",
    "        print(\"Possíveis causas: Dados realmente não cadastrados na Sólides ou falta de permissão no Token.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
