# ARQ-People Intelligence: Pipeline de Engenharia de Dados de RH

## üìã Vis√£o Geral
Este projeto consiste em um pipeline de **Engenharia de Dados (ETL)** robusto desenvolvido em Python para centralizar, limpar e estruturar dados de Recursos Humanos. O sistema orquestra a ingest√£o de dados de duas fontes distintas:
1.  **Arquivos N√£o-Estruturados (PDF):** Holerites, Recibos de F√©rias e 13¬∫ Sal√°rio.
2.  **API Externa (S√≥lides):** Dados cadastrais ricos e benef√≠cios.

O objetivo final √© alimentar um Data Warehouse (PostgreSQL) modelado em **Star Schema** para an√°lises de *People Analytics*.

---

## üèóÔ∏è Arquitetura do Projeto

O projeto segue uma arquitetura modular baseada em **Separation of Concerns (SoC)**, onde cada etapa do ETL possui responsabilidade √∫nica.

```text
/
‚îú‚îÄ‚îÄ input/                 # [√Årea de Staging Local] Recebe os PDFs brutos.
‚îú‚îÄ‚îÄ output/                # [√Årea Transiente] Armazena CSVs processados antes da carga.
‚îú‚îÄ‚îÄ src/                   # Pacote de c√≥digo fonte principal
‚îÇ   ‚îú‚îÄ‚îÄ database.py        # Factory de conex√£o com o Banco de Dados (Singleton pattern).
‚îÇ   ‚îú‚îÄ‚îÄ extract.py         # L√≥gica de Extra√ß√£o (OCR/Regex para PDF e Requests para API).
‚îÇ   ‚îú‚îÄ‚îÄ transform.py       # Limpeza, Tipagem Forte (Pandas) e Regras de Neg√≥cio.
‚îÇ   ‚îú‚îÄ‚îÄ load.py            # Carga no Banco (DDL, DML, Upserts e Tratamento de Erros).
‚îÇ   ‚îú‚îÄ‚îÄ utils.py           # Fun√ß√µes auxiliares (Sanitiza√ß√£o de texto e moeda).
‚îÇ   ‚îî‚îÄ‚îÄ constants.py       # Mapeamento est√°tico de Rubricas e Schemas.
‚îú‚îÄ‚îÄ main.py                # Orquestrador do Pipeline (Entry Point).
‚îú‚îÄ‚îÄ renomear_arquivo.py    # Utilit√°rio para padroniza√ß√£o de nomenclatura de arquivos.
‚îî‚îÄ‚îÄ .env                   # Vari√°veis de ambiente (Credenciais).
```

```mermaid
graph LR
    %% Defini√ß√£o de Estilos
    classDef storage fill:#e1f5fe,stroke:#01579b,stroke-width:2px;
    classDef file fill:#fff9c4,stroke:#fbc02d,stroke-width:2px;
    classDef python fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px;
    classDef bi fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px;
    classDef api fill:#ffe0b2,stroke:#e65100,stroke-width:2px;

    subgraph "Camada 1: Ingest√£o (Extract)"
        A[/"üìÇ input/*.pdf"/]:::file
        B(("‚òÅÔ∏è Solides API")):::api
    end

    subgraph "Camada 2: Processamento Modular (Python)"
        Orchestrator{{"üêç main.py (Orquestrador)"}}:::python
        
        %% Fluxo PDF
        A -->|L√™| Extract[["src/extract.py"]]:::python
        Extract -->|Dados Brutos| Transf[["src/transform.py"]]:::python
        
        %% Fluxo API
        B -->|Request| Extract
        
        %% Sa√≠da Transiente
        Transf -->|Gera Staging| C("üìÑ output/*.csv"):::file
        
        %% Carga
        Transf -->|DataFrames Limpos| Load[["src/load.py"]]:::python
        C -.->|Backup/Debug| Load
    end

    subgraph "Camada 3: Armazenamento (PostgreSQL DW)"
        Load -->|Upsert/Insert| DB[(PostgreSQL)]:::storage
        
        %% Tabelas
        DB --> T1[dim_colaboradores]:::storage
        DB --> T2[dim_calendario]:::storage
        DB --> T3[fato_folha_consolidada]:::storage
        DB --> T4[fato_folha_detalhada]:::storage
        DB --> T5[fato_beneficios_api]:::storage
    end

    subgraph "Camada 4: Analytics (Power BI)"
        T1 & T2 & T3 & T4 & T5 -->|Import Mode| PBI[Power Query]:::bi
        PBI --> Model{Modelagem Star Schema}:::bi
        Model --> Dash[üìä Dashboard People Analytics]:::bi

    %% Conex√µes do Orquestrador
    Orchestrator -.-> Extract
    Orchestrator -.-> Transf
    Orchestrator -.-> Load
    end
```
----

# üöÄ Detalhamento T√©cnico dos M√≥dulos

## 1. Extra√ß√£o (```src/extract.py```)


- **PDFs** : Utiliza a biblioteca ```pdfplumber``` para extra√ß√£o de texto bruto. **Aplica Express√µes Regulares (Regex)** complexas para identificar padr√µes de layout vari√°veis (Holerite Mensal vs. Recibo de F√©rias).

  - Estrat√©gia de Fallback: O extrator possui m√∫ltiplas camadas de regex. Se n√£o encontrar o padr√£o "Compet√™ncia: MM/AAAA", busca por "Data de Pagamento" ou "Per√≠odo de Gozo".
- **API**: Implementa pagina√ß√£o autom√°tica (```while loop```) para iterar sobre todos os endpoints da API da Solides, garantindo a extra√ß√£o completa da base de colaboradores.

## 2. Transforma√ß√£o (```src/transform.py```)

Focada em **Data Quality** e **Tipagem Forte.**

- **Sanitiza√ß√£o**: Converte strings monet√°rias brasileiras ('R$ 1.000,00') para objetos ``Decimal`` ou ``float`` limpos.

**Tratamento de Datas**: Converte strings para objetos ``datetime.date``, transformando valores inv√°lidos (``NaT``, ``nan``) explicitamente em ``None`` (NULL) para evitar erros no banco.

**Enriquecimento**: Padroniza nomes de rubricas baseados em um dicion√°rio de-para (``constants.py``).

## 3. Carga (``src/load.py``)

Utiliza SQLAlchemy e SQL puro para m√°xima performance e controle.

**Idempot√™ncia**: A carga de fatos utiliza a estrat√©gia *Delete-Insert* baseada na compet√™ncia. Isso permite reprocessar um m√™s inteiro sem duplicar dados.

**SCD Tipo 1 (Upsert)**: A dimens√£o de colaboradores utiliza ``INSERT ... ON CONFLICT DO UPDATE`` para garantir que o cadastro esteja sempre atualizado, mantendo o ID imut√°vel.

**Seguran√ßa de Tipos**: Implementa fun√ß√µes ``safe_cast`` no SQL (``CAST(NULLIF(..., '') AS NUMERIC``)) para blindar o banco contra strings vazias ou caracteres sujos vindos da fonte.

---

# üîí Pol√≠tica de Seguran√ßa e Reten√ß√£o de Dados

Este pipeline lida com Dados Pessoais Sens√≠veis (LGPD). As seguintes regras s√£o aplicadas via c√≥digo e processo:

1. **Pasta ``input/`` (PDFs)**: Destinada apenas para leitura moment√¢nea. Ap√≥s a execu√ß√£o do pipeline e valida√ß√£o, os arquivos devem ser exclu√≠dos ou movidos para um armazenamento frio seguro (Cold Storage/S3).

2. **Pasta ``output/`` (CSVs)**: Arquivos gerados apenas para debug e transporte (Staging). Devem ser **exclu√≠dos** imediatamente ap√≥s a confirma√ß√£o da carga no banco.

3. **Credenciais**: Nenhuma senha √© hardcoded. Tudo √© gerenciado via vari√°veis de ambiente (``.env``).

---
# ‚öôÔ∏è Como Executar
### Pr√©-requisitos
- Python 3.9+
- PostgreSQL 12+
- Depend√™ncias listadas em requirements.txt

### Passo a Passo

1. Configure o arquivo ```.env``` na raiz:
    ```text
    DB_USER=postgres
    DB_PASS=sua_senha
    DB_HOST=localhost
    DB_PORT=5432
    DB_NAME=dw_rh
    DB_SCHEMA=fopag_prod
    SOLIDES_API_TOKEN=seu_token
    ```
2. Coloque os PDFs na pasta ``input/.``.
3. (Opcional) Padronize os nomes dos arquivos:
   ```bash
   python renomear_arquivo.py
   ```
4. Execute o Pipeline principal:
   ```bash
   python main.py
   ```