# ARQ-People Intelligence: Pipeline de Engenharia de Dados de RH

## ğŸ“‹ VisÃ£o Geral
Este projeto consiste em um pipeline de **Engenharia de Dados (ETL)** robusto desenvolvido em Python para centralizar, limpar e estruturar dados de Recursos Humanos. O sistema orquestra a ingestÃ£o de dados de duas fontes distintas:
1.  **Arquivos NÃ£o-Estruturados (PDF):** Holerites, Recibos de FÃ©rias e 13Âº SalÃ¡rio.
2.  **API Externa (SÃ³lides):** Dados cadastrais ricos e benefÃ­cios.

O objetivo final Ã© alimentar um Data Warehouse (PostgreSQL) modelado em **Star Schema** para anÃ¡lises de *People Analytics*.

---

## ğŸ—ï¸ Arquitetura do Projeto

O projeto segue uma arquitetura modular baseada em **Separation of Concerns (SoC)**, onde cada etapa do ETL possui responsabilidade Ãºnica.

```text
/
â”œâ”€â”€ input/                 # [Ãrea de Staging Local] Recebe os PDFs brutos.
â”œâ”€â”€ output/                # [Ãrea Transiente] Armazena CSVs processados antes da carga.
â”œâ”€â”€ src/                   # Pacote de cÃ³digo fonte principal
â”‚   â”œâ”€â”€ database.py        # Factory de conexÃ£o com o Banco de Dados (Singleton pattern).
â”‚   â”œâ”€â”€ extract.py         # LÃ³gica de ExtraÃ§Ã£o (OCR/Regex para PDF e Requests para API).
â”‚   â”œâ”€â”€ transform.py       # Limpeza, Tipagem Forte (Pandas) e Regras de NegÃ³cio.
â”‚   â”œâ”€â”€ load.py            # Carga no Banco (DDL, DML, Upserts e Tratamento de Erros).
â”‚   â”œâ”€â”€ utils.py           # FunÃ§Ãµes auxiliares (SanitizaÃ§Ã£o de texto e moeda).
â”‚   â””â”€â”€ constants.py       # Mapeamento estÃ¡tico de Rubricas e Schemas.
â”œâ”€â”€ main.py                # Orquestrador do Pipeline (Entry Point).
â”œâ”€â”€ renomear_arquivo.py    # UtilitÃ¡rio para padronizaÃ§Ã£o de nomenclatura de arquivos.
â””â”€â”€ .env                   # VariÃ¡veis de ambiente (Credenciais).
```
---
# ğŸš€ Detalhamento TÃ©cnico dos MÃ³dulos

## 1. ExtraÃ§Ã£o (```src/extract.py```)


- **PDFs** : Utiliza a biblioteca ```pdfplumber``` para extraÃ§Ã£o de texto bruto. **Aplica ExpressÃµes Regulares (Regex)** complexas para identificar padrÃµes de layout variÃ¡veis (Holerite Mensal vs. Recibo de FÃ©rias).

  - EstratÃ©gia de Fallback: O extrator possui mÃºltiplas camadas de regex. Se nÃ£o encontrar o padrÃ£o "CompetÃªncia: MM/AAAA", busca por "Data de Pagamento" ou "PerÃ­odo de Gozo".
- **API**: Implementa paginaÃ§Ã£o automÃ¡tica (```while loop```) para iterar sobre todos os endpoints da API da Solides, garantindo a extraÃ§Ã£o completa da base de colaboradores.

## 2. TransformaÃ§Ã£o (```src/transform.py```)

Focada em **Data Quality** e **Tipagem Forte.**

- **SanitizaÃ§Ã£o**: Converte strings monetÃ¡rias brasileiras ('R$ 1.000,00') para objetos ``Decimal`` ou ``float`` limpos.

**Tratamento de Datas**: Converte strings para objetos ``datetime.date``, transformando valores invÃ¡lidos (``NaT``, ``nan``) explicitamente em ``None`` (NULL) para evitar erros no banco.

**Enriquecimento**: Padroniza nomes de rubricas baseados em um dicionÃ¡rio de-para (``constants.py``).

## 3. Carga (``src/load.py``)

Utiliza SQLAlchemy e SQL puro para mÃ¡xima performance e controle.

**IdempotÃªncia**: A carga de fatos utiliza a estratÃ©gia *Delete-Insert* baseada na competÃªncia. Isso permite reprocessar um mÃªs inteiro sem duplicar dados.

**SCD Tipo 1 (Upsert)**: A dimensÃ£o de colaboradores utiliza ``INSERT ... ON CONFLICT DO UPDATE`` para garantir que o cadastro esteja sempre atualizado, mantendo o ID imutÃ¡vel.

**SeguranÃ§a de Tipos**: Implementa funÃ§Ãµes ``safe_cast`` no SQL (``CAST(NULLIF(..., '') AS NUMERIC``)) para blindar o banco contra strings vazias ou caracteres sujos vindos da fonte.

---

# ğŸ”’ PolÃ­tica de SeguranÃ§a e RetenÃ§Ã£o de Dados

Este pipeline lida com Dados Pessoais SensÃ­veis (LGPD). As seguintes regras sÃ£o aplicadas via cÃ³digo e processo:

1. **Pasta ``input/`` (PDFs)**: Destinada apenas para leitura momentÃ¢nea. ApÃ³s a execuÃ§Ã£o do pipeline e validaÃ§Ã£o, os arquivos devem ser excluÃ­dos ou movidos para um armazenamento frio seguro (Cold Storage/S3).

2. **Pasta ``output/`` (CSVs)**: Arquivos gerados apenas para debug e transporte (Staging). Devem ser **excluÃ­dos** imediatamente apÃ³s a confirmaÃ§Ã£o da carga no banco.

3. **Credenciais**: Nenhuma senha Ã© hardcoded. Tudo Ã© gerenciado via variÃ¡veis de ambiente (``.env``).

---
# âš™ï¸ Como Executar
### PrÃ©-requisitos
- Python 3.9+
- PostgreSQL 12+
- DependÃªncias listadas em requirements.txt

### Passo a Passo

1. Configure o arquivo ```.env``` na raiz:
    ```text
    DB_USER=postgres
    DB_PASS=sua_senha
    DB_HOST=localhost
    DB_PORT=5432
    DB_NAME=dw_rh
    DB_SCHEMA=fopag_prod
    SOLIDES_API_TOKEN=seu_token
    ```
2. Coloque os PDFs na pasta ``input/.``.
3. (Opcional) Padronize os nomes dos arquivos:
   ```bash
   python renomear_arquivo.py
   ```
4. Execute o Pipeline principal:
   ```bash
   python main.py
   ```